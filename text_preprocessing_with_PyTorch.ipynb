{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# playing with pytorch and NLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T20:15:26.820585Z",
     "start_time": "2019-03-13T20:15:18.689125Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T20:19:18.848036Z",
     "start_time": "2019-03-13T20:19:18.839679Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'LABEL': [1, 2, 1, 1, 2],\n",
    "    'NAME': ['ram', 'shyam', 'jadu', 'madhu', 'ganesh']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T20:17:37.195144Z",
     "start_time": "2019-03-13T20:17:37.189800Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        return {'x_data': row.NAME, 'y_target': row.LABEL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T20:17:38.769572Z",
     "start_time": "2019-03-13T20:17:38.764968Z"
    }
   },
   "outputs": [],
   "source": [
    "mydata = MyDataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T20:17:41.030432Z",
     "start_time": "2019-03-13T20:17:40.988278Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_data': 'madhu', 'y_target': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata.__getitem__(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, token_to_index=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "          token_to_index (dict): a predefined mapping from token\n",
    "                           to index\n",
    "          add_unk (bool): flag to indicate whether token needs to be added\n",
    "          unk_token (str): the unk token to add into Vocabulary\n",
    "        \"\"\"\n",
    "\n",
    "        if token_to_index is None:\n",
    "            token_to_index = {}\n",
    "        self._token_to_index = token_to_index\n",
    "        self._idx_to_token = {\n",
    "            idx: token\n",
    "            for token, idx in self._token_to_index.items()\n",
    "        }\n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        \"\"\"\n",
    "        Return: Dictionary that can be serialized\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'token_to_index': self._token_to_index,\n",
    "            'add_unk': self._add_unk,\n",
    "            'unk_token': self.unk_token\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\"\n",
    "        Input: \n",
    "            contents (dict)\n",
    "        Return: \n",
    "            Instantiates the Vocabulary \n",
    "            from a serializable dictionary\n",
    "        \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            token (str): the item to be added into the Vocabulary\n",
    "        Return:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "\n",
    "        if token in self._token_to_index:\n",
    "            index = self._token_to_index[token]\n",
    "        else:\n",
    "            index = len(self._token_to_index)\n",
    "            self._token_to_index[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"\n",
    "        Retrieve the index associated with the token or the\n",
    "        UNK index if the token isn't present\n",
    "\n",
    "        Input:\n",
    "            token(str): the token to look up\n",
    "        Return:\n",
    "            index (int):  the index corresponding to the\n",
    "            token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having added into the\n",
    "            Vocabulary) for the UNK functionality\n",
    "        \"\"\"\n",
    "\n",
    "        if self.add_unk:\n",
    "            return self._token_to_index.get(token, self.unk_index)\n",
    "        else:\n",
    "            self._token_to_index[token]\n",
    "\n",
    "        def lookup_index(self, index):\n",
    "\n",
    "            if index not in self._idx_to_token:\n",
    "                raise KeyError(\n",
    "                    \"the index {} is not in the Vocabulary\".format(index))\n",
    "\n",
    "            return self._idx_to_token[index]\n",
    "\n",
    "        def __str__(self):\n",
    "            return \"<Vocabulary(size={})>\".format(len(self))\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self._token_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewVectorizer(object):\n",
    "    \"\"\"\n",
    "    The Vectorizer class coordinates with the Vocabulary class and \n",
    "    puts them into use\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, review_vocab, rating_vocab):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            review_vocab (Vocabulary): maps words to integers\n",
    "            review_rating (Vocabulary): maps class labels to integers\n",
    "        \"\"\"\n",
    "        self.review_vocab = review_vocab\n",
    "        self.rating_vocab = rating_vocab\n",
    "\n",
    "        def vectorize(self, review):\n",
    "            \"\"\"\n",
    "            create collapsed one hot vector for the review\n",
    "            input:\n",
    "                review (str): the review\n",
    "            return:\n",
    "                one_hot (np.ndarray): the collapsed one hot encoding\n",
    "            \"\"\"\n",
    "\n",
    "            one_hot = np.zeros(len(self.review_vocab), dtype=np.float32)\n",
    "\n",
    "            for token in review.split(\" \"):\n",
    "                if token not in string.punctuation:\n",
    "                    one_hot[self.review_vocab.lookup_token(token)]\n",
    "\n",
    "            return one_hot\n",
    "\n",
    "        @classmethod\n",
    "        def from_dataframe(cls, review_df, cutoff=25):\n",
    "            \"\"\"\n",
    "            Instantiate the vectorizer from the dataset dataframe\n",
    "            input:\n",
    "                review_df (pandas.DataFrame): the review dataset\n",
    "                cutoff (int): the parameter for frequency based\n",
    "                filtering\n",
    "            return:\n",
    "                an instance of ReviewVectorizer \n",
    "            \"\"\"\n",
    "\n",
    "            review_vocab = Vocabulary(add_unk=True)\n",
    "            rating_vocab = Vocabulary(add_unk=False)\n",
    "\n",
    "            # add ratings\n",
    "            for rating in sorted(set(review_df.rating)):\n",
    "                rating_vocab.add_token(rating)\n",
    "\n",
    "            # add top words if count > provided count\n",
    "            word_counts = Couner()  # dictionary\n",
    "\n",
    "            for review in review_df.review:\n",
    "                for words in review.split(\" \"):\n",
    "                    if word not in string.punctuation:\n",
    "                        word_counts[word] += 1\n",
    "\n",
    "            for word, count in word_counts.items():\n",
    "                if count > cutoff:\n",
    "                    review_vocab.add_token(word)\n",
    "\n",
    "            return cls(review_vocab, rating_vocab)\n",
    "\n",
    "        @classmethod\n",
    "        def from_serializable(cls, contents):\n",
    "            \"\"\"\n",
    "            Instantiate a ReviewVectorizer from a serializable \n",
    "            dictionary\n",
    "\n",
    "            input:\n",
    "                contents (dict): the serializable dictionary\n",
    "            return:\n",
    "                an instance of the ReviewVectorizer class\n",
    "            \"\"\"\n",
    "\n",
    "            review_vocab = Vocabulary.from_serializable(\n",
    "                contents['review_vocab'])\n",
    "            rating_vocab = Vocabulary.from_serializable(\n",
    "                contents['rating_vocab'])\n",
    "\n",
    "            return cls(review_vocab=review_vocab, rating_vocab=rating_vocab)\n",
    "\n",
    "        def to_serializable(self):\n",
    "            \"\"\"\n",
    "            Create the serializable dictionary for caching\n",
    "            return:\n",
    "                contents (dict): the serializable dictionary\n",
    "            \"\"\"\n",
    "\n",
    "            return {\n",
    "                'review_vocab': self.review_vocab.to_serializable(),\n",
    "                'rating_vocab': self.rating_vocab.to_serializable()\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset,\n",
    "                     batch_size,\n",
    "                     shuffle=True,\n",
    "                     drop_last=True,\n",
    "                     device='cpu'):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader.\n",
    "    It will ensure each tensor is on the right device location\n",
    "    \"\"\"\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T20:22:36.564659Z",
     "start_time": "2019-03-13T20:22:36.552377Z"
    }
   },
   "outputs": [],
   "source": [
    "x = {'a': 37, 'b': 42, 'c': 927}\n",
    "\n",
    "y = 'hello ' 'world'\n",
    "z = 'hello ' + 'world'\n",
    "a = 'hello {}'.format('world')\n",
    "\n",
    "\n",
    "class foo(object):\n",
    "    def f(self):\n",
    "        return 37 * -+2\n",
    "\n",
    "    def g(self, x, y=42):\n",
    "        return y\n",
    "\n",
    "\n",
    "def f(a):\n",
    "    return 37 + -+a[42 - x:y**3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "183.85px",
    "left": "1020px",
    "right": "20px",
    "top": "120px",
    "width": "326px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
