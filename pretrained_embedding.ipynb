{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Embeddings\n",
    "\n",
    "The following content is heavily based on [this](https://github.com/joosthub/PyTorchNLPBook/blob/master/chapters/chapter_5/5_1_Pretrained_Embeddings.ipynb) notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is `annoy`?\n",
    "\n",
    "`Annoy` (Approximate Nearest Neighbors Oh Yeah) is a  C++/Python optimized for memory usage and loading/saving to disk.\n",
    "\n",
    "The current implementation for finding k nearest neighbors in a vector space in gensim has linear complexity via brute force in the number of indexed documents, although with extremely low constant factors. The retrieved results are exact, which is an overkill in many applications: approximate results retrieved in sub-linear time may be enough. Annoy can find approximate nearest neighbors much faster.\n",
    "\n",
    "- [Annoy_Tutorial](https://markroxor.github.io/gensim/static/notebooks/annoytutorial.html)\n",
    "- [Blog](https://medium.com/@kevin_yang/simple-approximate-nearest-neighbors-in-python-with-annoy-and-lmdb-e8a701baf905)\n",
    "- [Code Example](https://github.com/spotify/annoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from annoy import AnnoyIndex\n",
    "import numpy as np\n",
    "from typing import List, Set, Dict, Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTrainedEmbeddings(object):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 word_to_index:Dict[str, int], \n",
    "                 word_vectors:List[np.ndarray]):\n",
    "        \n",
    "        self.word_to_index = word_to_index\n",
    "        self.word_vectors = word_vectors\n",
    "        self.index_to_words = {idx: word for word, idx in tqdm(self.word_to_index.items())}\n",
    "        \n",
    "        # Length of item vector that will be indexed\n",
    "        self.index = AnnoyIndex(len(word_vectors[0]), metric='euclidean')\n",
    "        print(\"Building Annoy Index...\")\n",
    "        for _, i in tqdm(self.word_to_index.items()):\n",
    "            self.index.add_item(i, self.word_vectors[i])\n",
    "            \n",
    "        self.index.build(50) # 50 trees\n",
    "        print(\"Finished!!\")\n",
    "        \n",
    "    @classmethod\n",
    "    def from_embedding_file(cls, embedding_file:str):\n",
    "        \"\"\"\n",
    "        Instantiate from the embedding file\n",
    "        \n",
    "        Vector file should be of the format:\n",
    "            word0 x0_0 x0_1 x0_2 x0_3 ... x0_N\n",
    "            word1 x1_0 x1_1 x1_2 x1_3 ... x1_N\n",
    "        \n",
    "        Returns:\n",
    "            Instance of the PretrainedEmbeddings\n",
    "        \"\"\"\n",
    "        \n",
    "        word_to_index = {}\n",
    "        word_vectors = []\n",
    "        \n",
    "        print(\"Processing Embedding file ...\")\n",
    "        with open(embedding_file, 'r') as fp:\n",
    "            for line in tqdm(fp.readlines()):\n",
    "                \n",
    "                line = line.split(\" \")\n",
    "                word = line[0]\n",
    "                emb = np.array([float(x) for x in line[1:]])\n",
    "                \n",
    "                word_to_index[word] = len(word_to_index)\n",
    "                word_vectors.append(emb)\n",
    "                \n",
    "        return cls(word_to_index, word_vectors)\n",
    "    \n",
    "    \n",
    "    def get_embedding(self, word:str):\n",
    "        \"\"\"Given word, returns the embedding vector \n",
    "        \n",
    "        Return:\n",
    "            an embedding (np.ndarray)\n",
    "        \"\"\"\n",
    "        return self.word_vectors[self.word_to_index[word]]\n",
    "    \n",
    "    def get_closest_to_vectors(self, vector:np.ndarray, n:int=1):\n",
    "        \"\"\"Given a vector, return its n nearest neighbours \n",
    "        \n",
    "        Args: \n",
    "            vector (np.ndarray): should match the size of the vectors in the Annoy index\n",
    "            n (int): the number of neighbours to return\n",
    "            \n",
    "        Returns:\n",
    "            [str, str, str, ,...]: Words, those are nearest to the given word\n",
    "                                   The words are not ordered by distance\n",
    "                                   \n",
    "        \"\"\"\n",
    "        nn_indices = self.index.get_nns_by_vector(vector, n)\n",
    "        \n",
    "        ls_word = [self.index_to_words[neighbours_idx] for neighbours_idx in nn_indices]\n",
    "        return ls_word\n",
    "    \n",
    "    def compute_and_print_analogy(self, word1, word2, word3):\n",
    "        \n",
    "        emb1 = self.get_embedding(word1)\n",
    "        emb2 = self.get_embedding(word2)\n",
    "        emb3 = self.get_embedding(word3)\n",
    "        \n",
    "        # let's compute the 4th words mebedding!!\n",
    "        # idea: w2 - w1 = w4 - w3\n",
    "        spatial_relationship = emb2 - emb1\n",
    "        emb4 = emb3 + spatial_relationship\n",
    "        \n",
    "        # get closest neighbours\n",
    "        closest_words = self.get_closest_to_vectors(emb4, 4)\n",
    "        existing_words = set([word1, word2, word3])\n",
    "        closest_words = [word for word in closest_words if word not in existing_words]\n",
    "        \n",
    "        if len(closest_words) == 0:\n",
    "            print(\"Couldn't find closest words\")\n",
    "            print(f\"{word1} : {word2} :: {word3} : ??\")\n",
    "        \n",
    "        else:\n",
    "            for w4 in closest_words:\n",
    "                print(f\"{word1} : {word2} :: {word3} : {w4}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_file = \"data/embeddinggs/glove.6B.100d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Embedding file ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:13<00:00, 29594.60it/s]\n",
      "100%|██████████| 400000/400000 [00:00<00:00, 2275564.81it/s]\n",
      "  5%|▌         | 21736/400000 [00:00<00:03, 108667.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Annoy Index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:03<00:00, 110584.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!!\n"
     ]
    }
   ],
   "source": [
    "embeddings = PreTrainedEmbeddings.from_embedding_file(emb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : he :: woman : she\n",
      "man : he :: woman : never\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy('man', 'he', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fly : plane :: sail : ship\n",
      "fly : plane :: sail : vessel\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy('fly', 'plane', 'sail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat : kitten :: dog : puppy\n",
      "cat : kitten :: dog : toddler\n",
      "cat : kitten :: dog : sleds\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy('cat', 'kitten', 'dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue : color :: dog : cat\n",
      "blue : color :: dog : animal\n",
      "blue : color :: dog : breed\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy('blue', 'color', 'dog')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leg : legs :: hand : fingers\n",
      "leg : legs :: hand : ears\n",
      "leg : legs :: hand : stick\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy('leg', 'legs', 'hand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy : love :: girl : boy\n",
      "happy : love :: girl : woman\n",
      "happy : love :: girl : mother\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy('happy', 'love', 'girl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netpool import pretrainedEmbedding as pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Embedding file ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:13<00:00, 29355.61it/s]\n",
      "100%|██████████| 400000/400000 [00:00<00:00, 2272688.80it/s]\n",
      "  5%|▌         | 21333/400000 [00:00<00:03, 106647.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Annoy Index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:03<00:00, 106634.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!!\n"
     ]
    }
   ],
   "source": [
    "embss = pe.PreTrainedEmbeddings.from_embedding_file(emb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embss.compute_and_print_analogy('man', 'he', 'woman')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
