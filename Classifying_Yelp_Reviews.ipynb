{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Review - Sentiment Classification\n",
    "- [Book: NLP with Pytorch](https://nbviewer.jupyter.org/github/joosthub/PyTorchNLPBook/blob/master/chapters/chapter_3/3_5_Classifying_Yelp_Review_Sentiment.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Tuple, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## General Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Hyperparameter and other meta information setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "args = Namespace(frequency_cutoff = 25, \n",
    "                 model_state_file='model_yelp_classification.pth',\n",
    "                 file_csv = 'data/yelp/reviews_with_splits_lite.csv',\n",
    "                 save_dir='model_storage/ch3/yelp/',\n",
    "                 vectorizer_file = 'vectorizer_yelp_review.json',\n",
    "                 batch_size=128,\n",
    "                 early_stopping_criteria = 5,\n",
    "                 learning_rate = 0.001,\n",
    "                 num_epochs = 3,\n",
    "                 seed = 2019,\n",
    "                 catch_keyboard_interrupt = True,\n",
    "                 cuda = True,\n",
    "                 expand_filepaths_to_save_dir = True,\n",
    "                 reload_from_files = False\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodel_storage/ch3/yelp/vectorizer_yelp_review.json\n",
      "\tmodel_storage/ch3/yelp/model_yelp_classification.pth\n",
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Vectorization Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## The Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Process text and extract vocab for mapping\"\"\"\n",
    "    def __init__(self, \n",
    "                 token_to_idx:Dict=None, \n",
    "                 add_unk:bool=True, \n",
    "                 unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx: a pre-existing map of token to indices\n",
    "            add_unk: a flag to indicate whether to add unknown tokens\n",
    "            unk_token: the UNK token to add into the vocabulary\n",
    "        \"\"\"\n",
    "        \n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        \n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx : token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        \n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "            \n",
    "    \n",
    "    def add_token(self, token:str):\n",
    "        \"\"\"Update mapping dictionary\n",
    "        Return:\n",
    "            the intiger index corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:return self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "            return index\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        \"\"\"returns a dict that can be serialized\"\"\"\n",
    "        return {'token_to_idx':self._token_to_idx, \n",
    "                'add_unk': self._add_unk, \n",
    "                'unk_token': self._unk_token}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, content):\n",
    "        \"\"\"instantiates the vocub from a serializable dict\"\"\"\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def add_many(self, tokens:List[str])->List[int]:\n",
    "        \"\"\"Add list of tokens into the Vocabulary\n",
    "            \n",
    "        \"\"\"\n",
    "        return [self.add_token(tok) for tok in tokens]\n",
    "    \n",
    "    def lookup_token(self, token:str)->int:\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >=0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "    \n",
    "    def lookup_index(self, index:int)->str:\n",
    "        \"\"\"Return the token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(f\"index: {index} not in the Vocabulary\")\n",
    "        \n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Vocabulary Size:{len(self)}\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## The Vectorizer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ReviewVectorizer(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 review_vocab: Vocabulary, \n",
    "                 rating_vocab: Vocabulary):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            review_vocab: maps words to integers\n",
    "            rating_vocab: maps class labels to integers\n",
    "        \"\"\"\n",
    "        self.review_vocab = review_vocab\n",
    "        self.rating_vocab = rating_vocab\n",
    "    \n",
    "    def vectorizer(self, review:str):\n",
    "        \"\"\"Creates a collapsed one-hot vector for the review\n",
    "        Args:\n",
    "            review (str): the review\n",
    "        Returns:\n",
    "            one_hot: the collapsed one hot encoding\n",
    "        \"\"\"\n",
    "        # review_vocab: is the entire review vocabulary \n",
    "        one_hot = np.zeros(len(self.review_vocab), dtype=np.float32)\n",
    "        for token in review.split(\" \"):\n",
    "            if token not in string.punctuation:\n",
    "                one_hot[self.review_vocab.lookup_token(token)] = 1\n",
    "        \n",
    "        return one_hot\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, review_df, cutoff=25):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            review_df (pandas.DataFrame): the review dataset\n",
    "            cutoff (int): the parameter for frequency-based filtering\n",
    "        Returns:\n",
    "            an instance of the ReviewVectorizer\n",
    "        \"\"\"\n",
    "        review_vocab = Vocabulary(add_unk=True)\n",
    "        rating_vocab = Vocabulary(add_unk=False)\n",
    "        \n",
    "        # add ratings\n",
    "        for rating in sorted(set(review_df.rating)):\n",
    "            rating_vocab.add_token(rating)\n",
    "        \n",
    "        # Add top words if count > provided count\n",
    "        word_counts = Counter()\n",
    "        for review in review_df.review:\n",
    "            for word in review.split(\" \"):\n",
    "                if word not in string.punctuation:\n",
    "                    word_counts[word] += 1\n",
    "                    \n",
    "        for word, count in word_counts.items():\n",
    "            if count > cutoff:\n",
    "                review_vocab.add_token(word)\n",
    "                \n",
    "        return cls(review_vocab, rating_vocab)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents:Dict):\n",
    "        \"\"\"Instantiate a ReviewVectorizer from a serializable dictionary\n",
    "        \n",
    "        Args:\n",
    "            contents (dict): the serializable dictionary\n",
    "        Returns:\n",
    "            an instance of the ReviewVectorizer class\n",
    "        \"\"\"\n",
    "        review_vocab = Vocabulary.from_serializable(contents['review_vocab'])\n",
    "        rating_vocab = Vocabulary.from_serializable(contents['rating_vocab'])\n",
    "        \n",
    "        return cls(review_vocab, rating_vocab)\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        \"\"\"Create the serializable dictionary for caching\n",
    "        \n",
    "        Returns:\n",
    "            contents (dict): the serializable dictionary\n",
    "        \"\"\"\n",
    "        return {'review_vocab': self.review_vocab.to_serializable(),\n",
    "                'rating_vocab': self.rating_vocab.to_serializable()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 review_df:pd.DataFrame, \n",
    "                 vectorizer: ReviewVectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            review_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (ReviewVectorizer): vectorizer instantiated from dataset\n",
    "        \"\"\"\n",
    "        self.review_df = review_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        self.train_df = self.review_df[self.review_df.split == \"train\"]\n",
    "        self.train_size = len(self.train_df)\n",
    "        \n",
    "        \n",
    "        self.val_df = self.review_df[self.review_df.split == \"val\"]\n",
    "        self.validation_size = len(self.val_df)\n",
    "        \n",
    "        self.test_df = self.review_df[self.review_df.split == \"test\"]\n",
    "        self.test_size = len(self.test_df)\n",
    "        \n",
    "        self._lookup_split = {\"train\": (self.train_df, self.train_size), \n",
    "                              \"val\": (self.val_df, self.validation_size), \n",
    "                              \"test\": (self.test_df, self.test_size)\n",
    "                             }\n",
    "        \n",
    "        self.set_split(\"train\")\n",
    "    \n",
    "    def set_split(self, \n",
    "                  split:str=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \n",
    "        \n",
    "        Args:\n",
    "            split (str): one of \"train\", \"val\", or \"test\"\n",
    "        \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_split[split]\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, \n",
    "                                         file_review:str):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        \n",
    "        Args:\n",
    "            file_review (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of ReviewDataset\n",
    "        \"\"\"\n",
    "        review_df = pd.read_csv(file_review)\n",
    "        train_review_df = review_df[review_df.split == \"train\"]\n",
    "        \n",
    "        return cls(review_df, ReviewVectorizer.from_dataframe(train_review_df))\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, \n",
    "                                         file_review:str, \n",
    "                                         file_vectorizer:str):\n",
    "        \"\"\"Load dataset and the corresponding vectorizer. \n",
    "        Used in the case in the vectorizer has been cached for re-use\n",
    "        \n",
    "        Args:\n",
    "            review_csv (str): location of the dataset\n",
    "            vectorizer_filepath (str): location of the saved vectorizer\n",
    "        Returns:\n",
    "            an instance of ReviewDataset\n",
    "        \"\"\"\n",
    "        review_df = pd.read_csv(file_review)\n",
    "        vectorizer = cls.load_vectorizer_only(file_vectorizer)\n",
    "        return cls(review_df, vectorizer)\n",
    "    \n",
    "    \n",
    "    def save_vectorizer(self, file_vectorizer:str):\n",
    "        \"\"\"saves the vectorizer to disk using json\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location to save the vectorizer\n",
    "        \"\"\"\n",
    "        with open(file_vectorizer, \"w\") as fout:\n",
    "            json.dump(self._vectorizer.to_serializable(), fout)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(file_vectorizer:str):\n",
    "        \"\"\"a static method for loading the vectorizer from file\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location of the serialized vectorizer\n",
    "        Returns:\n",
    "            an instance of ReviewVectorizer\n",
    "        \"\"\"\n",
    "        with open(file_vectorize, \"r\") as fin:\n",
    "            return ReviewVectorizer.from_serializable(json.load(fin))\n",
    "    \n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        \"\"\"Returns the vectorizer\"\"\"\n",
    "        return self._vectorizer\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def __getitem__(self, index:int):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        \n",
    "        row = self._target_df.iloc[index]\n",
    "        review_vector = self._vectorizer.vectorizer(row.review)\n",
    "        \n",
    "        rating_index = self._vectorizer.rating_vocab.lookup_token(row.rating)\n",
    "    \n",
    "        return {'x_data': review_vector, \n",
    "                \"y_target\": rating_index}\n",
    "    \n",
    "    def get_num_batches(self, batch_size:int):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generate_batches(datase:Dataset, \n",
    "                     batch_size:int, \n",
    "                     shuffle:bool=True, \n",
    "                     drop_last:bool=True, \n",
    "                     device:str=\"cpu\"):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the correct device location.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataloader = DataLoader(dataset = dataset, \n",
    "                            batch_size=batch_size,\n",
    "                            shuffle = shuffle,\n",
    "                            drop_last = drop_last)\n",
    "    \n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        \n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## The Model: Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ReviewClassifier(nn.Module):\n",
    "    \"\"\"A simple perseptron based classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features:int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_features (int): size of the input features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=num_features, \n",
    "                             out_features = 1)\n",
    "        \n",
    "    def forward(self, \n",
    "                x_in:torch.Tensor, \n",
    "                apply_sigmoid:bool = False):\n",
    "        \n",
    "        \"\"\"The forward pass of the classifier\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor\n",
    "                                x_in.shape = (batch_size, num_features)\n",
    "            apply_sigmoid (bool): a flag for sigmoid activation\n",
    "                                should be False if used with cross entropy\n",
    "                                losses\n",
    "        Returns:\n",
    "            the resulting tensor.\n",
    "            shape: (batch_size,)\n",
    "        \n",
    "        \"\"\"\n",
    "        y_out = self.fc1(x_in).squeeze()\n",
    "        if apply_sigmoid: y_out = torch.sigmoid(y_out)\n",
    "        \n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1187,  0.2110],\n",
       "        [ 0.7463, -0.6136],\n",
       "        [-0.1186,  1.5565],\n",
       "        [ 1.3662,  1.0199],\n",
       "        [ 2.4644,  1.1630]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(5, 2, dtype=torch.float32)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.3541],\n",
       "         [ 0.4962],\n",
       "         [-1.0901],\n",
       "         [-0.1113],\n",
       "         [ 0.3172]], grad_fn=<AddmmBackward>), torch.Size([5, 1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = nn.Linear(2,1)(a)\n",
    "o, o.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3541,  0.4962, -1.0901, -0.1113,  0.3172],\n",
       "       grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Notes\n",
    "\n",
    "### Q. Why we don't use sigmoid with Cross Entropy\n",
    "\n",
    "\n",
    "### Q. What `torch.squeeze()` does? \n",
    "\n",
    "Returns a tensor with all the dimensions of input of size 1 removed.\n",
    "\n",
    "For example, if input is of shape: $(A \\times 1 \\times B \\times C \\times 1 \\times D)$ then the out tensor will be of shape: $(A \\times B \\times C \\times D)$.\n",
    "\n",
    "When `dim` is given, a `squeeze` operation is done only in the given dimension. If input is of shape: $(A \\times 1 \\times B)$, `squeeze(input, 0)` leaves the tensor unchanged, but `squeeze(input, 1)` will squeeze the tensor to the shape $(A \\times B)$.\n",
    "\n",
    "```py\n",
    ">>> x = torch.zeros(2, 1, 2, 1, 2)\n",
    ">>> x.size()\n",
    "torch.Size([2, 1, 2, 1, 2])\n",
    ">>> y = torch.squeeze(x)\n",
    ">>> y.size()\n",
    "torch.Size([2, 2, 2])\n",
    ">>> y = torch.squeeze(x, 0)\n",
    ">>> y.size()\n",
    "torch.Size([2, 1, 2, 1, 2])\n",
    ">>> y = torch.squeeze(x, 1)\n",
    ">>> y.size()\n",
    "torch.Size([2, 2, 1, 2])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Training Routine\n",
    "\n",
    "### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, \n",
    "                       model: ReviewClassifier, \n",
    "                       train_state: Dict):\n",
    "    \"\"\"Handle the training state updates.\n",
    "\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "\n",
    "    :param args: main arguments\n",
    "    :param model: model to train\n",
    "    :param train_state: a dictionary representing the training state values\n",
    "    :returns:\n",
    "        a new train_state\n",
    "    \"\"\"\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # If loss worsened\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_target = y_target.cpu()\n",
    "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()#.max(dim=1)[1]\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset and creating vectorizer\n"
     ]
    }
   ],
   "source": [
    "if args.reload_from_files:\n",
    "    # training from a checkpoint\n",
    "    print(\"Loading dataset and vectorizer\")\n",
    "    dataset = ReviewDataset.load_dataset_and_load_vectorizer(args.file_csv,\n",
    "                                                            args.vectorizer_file)\n",
    "else:\n",
    "    print(\"Loading dataset and creating vectorizer\")\n",
    "    # create dataset and vectorizer\n",
    "    dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.file_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vectorizer = dataset.get_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "classifier = ReviewClassifier(num_features=len(vectorizer.review_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training routine:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "split=train:   0%|          | 0/306 [00:00<?, ?it/s]\u001b[A\n",
      "split=val:   0%|          | 0/65 [00:00<?, ?it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = optim.Adam(classifier.parameters(), \n",
    "                       lr=args.learning_rate)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, \n",
    "                                                 mode = 'min', \n",
    "                                                 factor = 0.5, \n",
    "                                                 patience = 1)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm(desc='training routine', \n",
    "                  total=args.num_epochs,\n",
    "                  position=0)\n",
    "\n",
    "\n",
    "dataset.set_split(\"train\")\n",
    "\n",
    "train_bar = tqdm(desc='split=train',\n",
    "                  total=dataset.get_num_batches(args.batch_size), \n",
    "                  position=1, \n",
    "                  leave=True)\n",
    "\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='split=val',\n",
    "                total=dataset.get_num_batches(args.batch_size), \n",
    "                position=1, \n",
    "                leave=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "loss_func = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(classifier.parameters(), \n",
    "                       lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, \n",
    "                                                 mode = 'min', \n",
    "                                                 factor = 0.5, \n",
    "                                                 patience = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_state = make_train_state(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "epoch_bar = tqdm(desc='training routine', \n",
    "                  total=args.num_epochs,\n",
    "                  position=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset.set_split(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:   0%|          | 0/306 [00:00<?, ?it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "train_bar = tqdm(desc='split=train',\n",
    "                  total=dataset.get_num_batches(args.batch_size), \n",
    "                  position=1, \n",
    "                  leave=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=val:   0%|          | 0/65 [00:00<?, ?it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='split=val',\n",
    "                total=dataset.get_num_batches(args.batch_size), \n",
    "                position=1, \n",
    "                leave=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\t epoch:0/3\t batch_index:0\t train_acc: 44.53\t train_loss: 0.696\n",
      "train\t epoch:0/3\t batch_index:1\t train_acc: 47.27\t train_loss: 0.6935\n",
      "train\t epoch:0/3\t batch_index:2\t train_acc: 50.0\t train_loss: 0.6928\n",
      "train\t epoch:0/3\t batch_index:3\t train_acc: 49.61\t train_loss: 0.6916\n",
      "train\t epoch:0/3\t batch_index:4\t train_acc: 50.31\t train_loss: 0.6886\n",
      "train\t epoch:0/3\t batch_index:5\t train_acc: 50.91\t train_loss: 0.6871\n",
      "train\t epoch:0/3\t batch_index:6\t train_acc: 50.89\t train_loss: 0.6845\n",
      "train\t epoch:0/3\t batch_index:7\t train_acc: 51.17\t train_loss: 0.6835\n",
      "train\t epoch:0/3\t batch_index:8\t train_acc: 51.74\t train_loss: 0.6819\n",
      "train\t epoch:0/3\t batch_index:9\t train_acc: 51.8\t train_loss: 0.6815\n",
      "train\t epoch:0/3\t batch_index:10\t train_acc: 52.34\t train_loss: 0.6797\n",
      "train\t epoch:0/3\t batch_index:11\t train_acc: 53.58\t train_loss: 0.6771\n",
      "train\t epoch:0/3\t batch_index:12\t train_acc: 53.37\t train_loss: 0.6774\n",
      "train\t epoch:0/3\t batch_index:13\t train_acc: 54.19\t train_loss: 0.6766\n",
      "train\t epoch:0/3\t batch_index:14\t train_acc: 54.84\t train_loss: 0.6746\n",
      "train\t epoch:0/3\t batch_index:15\t train_acc: 55.52\t train_loss: 0.673\n",
      "train\t epoch:0/3\t batch_index:16\t train_acc: 56.48\t train_loss: 0.6713\n",
      "train\t epoch:0/3\t batch_index:17\t train_acc: 57.12\t train_loss: 0.6695\n",
      "train\t epoch:0/3\t batch_index:18\t train_acc: 57.4\t train_loss: 0.6689\n",
      "train\t epoch:0/3\t batch_index:19\t train_acc: 58.05\t train_loss: 0.6672\n",
      "train\t epoch:0/3\t batch_index:20\t train_acc: 59.04\t train_loss: 0.6658\n",
      "train\t epoch:0/3\t batch_index:21\t train_acc: 59.45\t train_loss: 0.6646\n",
      "train\t epoch:0/3\t batch_index:22\t train_acc: 60.05\t train_loss: 0.6625\n",
      "train\t epoch:0/3\t batch_index:23\t train_acc: 60.64\t train_loss: 0.6612\n",
      "train\t epoch:0/3\t batch_index:24\t train_acc: 61.03\t train_loss: 0.6605\n",
      "train\t epoch:0/3\t batch_index:25\t train_acc: 61.66\t train_loss: 0.6587\n",
      "train\t epoch:0/3\t batch_index:26\t train_acc: 62.09\t train_loss: 0.6576\n",
      "train\t epoch:0/3\t batch_index:27\t train_acc: 62.81\t train_loss: 0.6561\n",
      "train\t epoch:0/3\t batch_index:28\t train_acc: 63.25\t train_loss: 0.6549\n",
      "train\t epoch:0/3\t batch_index:29\t train_acc: 63.7\t train_loss: 0.6539\n",
      "train\t epoch:0/3\t batch_index:30\t train_acc: 64.29\t train_loss: 0.6521\n",
      "train\t epoch:0/3\t batch_index:31\t train_acc: 64.87\t train_loss: 0.6509\n",
      "train\t epoch:0/3\t batch_index:32\t train_acc: 65.32\t train_loss: 0.6495\n",
      "train\t epoch:0/3\t batch_index:33\t train_acc: 65.83\t train_loss: 0.648\n",
      "train\t epoch:0/3\t batch_index:34\t train_acc: 66.36\t train_loss: 0.6466\n",
      "train\t epoch:0/3\t batch_index:35\t train_acc: 66.56\t train_loss: 0.6455\n",
      "train\t epoch:0/3\t batch_index:36\t train_acc: 67.02\t train_loss: 0.6439\n",
      "train\t epoch:0/3\t batch_index:37\t train_acc: 67.33\t train_loss: 0.6426\n",
      "train\t epoch:0/3\t batch_index:38\t train_acc: 67.81\t train_loss: 0.6412\n",
      "train\t epoch:0/3\t batch_index:39\t train_acc: 68.2\t train_loss: 0.6399\n",
      "train\t epoch:0/3\t batch_index:40\t train_acc: 68.39\t train_loss: 0.6394\n",
      "train\t epoch:0/3\t batch_index:41\t train_acc: 68.71\t train_loss: 0.6382\n",
      "train\t epoch:0/3\t batch_index:42\t train_acc: 69.1\t train_loss: 0.6369\n",
      "train\t epoch:0/3\t batch_index:43\t train_acc: 69.3\t train_loss: 0.6358\n",
      "train\t epoch:0/3\t batch_index:44\t train_acc: 69.57\t train_loss: 0.6351\n",
      "train\t epoch:0/3\t batch_index:45\t train_acc: 69.77\t train_loss: 0.6344\n",
      "train\t epoch:0/3\t batch_index:46\t train_acc: 70.06\t train_loss: 0.6332\n",
      "train\t epoch:0/3\t batch_index:47\t train_acc: 70.31\t train_loss: 0.6319\n",
      "train\t epoch:0/3\t batch_index:48\t train_acc: 70.62\t train_loss: 0.6307\n",
      "train\t epoch:0/3\t batch_index:49\t train_acc: 70.87\t train_loss: 0.6295\n",
      "train\t epoch:0/3\t batch_index:50\t train_acc: 71.06\t train_loss: 0.6284\n",
      "train\t epoch:0/3\t batch_index:51\t train_acc: 71.26\t train_loss: 0.6273\n",
      "train\t epoch:0/3\t batch_index:52\t train_acc: 71.45\t train_loss: 0.6262\n",
      "train\t epoch:0/3\t batch_index:53\t train_acc: 71.59\t train_loss: 0.6252\n",
      "train\t epoch:0/3\t batch_index:54\t train_acc: 71.85\t train_loss: 0.6241\n",
      "train\t epoch:0/3\t batch_index:55\t train_acc: 72.06\t train_loss: 0.6228\n",
      "train\t epoch:0/3\t batch_index:56\t train_acc: 72.16\t train_loss: 0.6219\n",
      "train\t epoch:0/3\t batch_index:57\t train_acc: 72.29\t train_loss: 0.6208\n",
      "train\t epoch:0/3\t batch_index:58\t train_acc: 72.48\t train_loss: 0.6201\n",
      "train\t epoch:0/3\t batch_index:59\t train_acc: 72.62\t train_loss: 0.6192\n",
      "train\t epoch:0/3\t batch_index:60\t train_acc: 72.8\t train_loss: 0.6181\n",
      "train\t epoch:0/3\t batch_index:61\t train_acc: 73.05\t train_loss: 0.6167\n",
      "train\t epoch:0/3\t batch_index:62\t train_acc: 73.18\t train_loss: 0.6158\n",
      "train\t epoch:0/3\t batch_index:63\t train_acc: 73.3\t train_loss: 0.6147\n",
      "train\t epoch:0/3\t batch_index:64\t train_acc: 73.4\t train_loss: 0.6139\n",
      "train\t epoch:0/3\t batch_index:65\t train_acc: 73.51\t train_loss: 0.6128\n",
      "train\t epoch:0/3\t batch_index:66\t train_acc: 73.55\t train_loss: 0.6123\n",
      "train\t epoch:0/3\t batch_index:67\t train_acc: 73.64\t train_loss: 0.6113\n",
      "train\t epoch:0/3\t batch_index:68\t train_acc: 73.82\t train_loss: 0.6103\n",
      "train\t epoch:0/3\t batch_index:69\t train_acc: 73.96\t train_loss: 0.6092\n",
      "train\t epoch:0/3\t batch_index:70\t train_acc: 74.08\t train_loss: 0.6082\n",
      "train\t epoch:0/3\t batch_index:71\t train_acc: 74.18\t train_loss: 0.6074\n",
      "train\t epoch:0/3\t batch_index:72\t train_acc: 74.34\t train_loss: 0.6064\n",
      "train\t epoch:0/3\t batch_index:73\t train_acc: 74.48\t train_loss: 0.6054\n",
      "train\t epoch:0/3\t batch_index:74\t train_acc: 74.65\t train_loss: 0.6044\n",
      "train\t epoch:0/3\t batch_index:75\t train_acc: 74.63\t train_loss: 0.6042\n",
      "train\t epoch:0/3\t batch_index:76\t train_acc: 74.84\t train_loss: 0.603\n",
      "train\t epoch:0/3\t batch_index:77\t train_acc: 74.93\t train_loss: 0.602\n",
      "train\t epoch:0/3\t batch_index:78\t train_acc: 75.07\t train_loss: 0.6008\n",
      "train\t epoch:0/3\t batch_index:79\t train_acc: 75.15\t train_loss: 0.5999\n",
      "train\t epoch:0/3\t batch_index:80\t train_acc: 75.28\t train_loss: 0.599\n",
      "train\t epoch:0/3\t batch_index:81\t train_acc: 75.39\t train_loss: 0.5984\n",
      "train\t epoch:0/3\t batch_index:82\t train_acc: 75.5\t train_loss: 0.5975\n",
      "train\t epoch:0/3\t batch_index:83\t train_acc: 75.59\t train_loss: 0.5964\n",
      "train\t epoch:0/3\t batch_index:84\t train_acc: 75.72\t train_loss: 0.5955\n",
      "train\t epoch:0/3\t batch_index:85\t train_acc: 75.78\t train_loss: 0.5947\n",
      "train\t epoch:0/3\t batch_index:86\t train_acc: 75.94\t train_loss: 0.5936\n",
      "train\t epoch:0/3\t batch_index:87\t train_acc: 76.02\t train_loss: 0.5928\n",
      "train\t epoch:0/3\t batch_index:88\t train_acc: 76.14\t train_loss: 0.5921\n",
      "train\t epoch:0/3\t batch_index:89\t train_acc: 76.2\t train_loss: 0.5913\n",
      "train\t epoch:0/3\t batch_index:90\t train_acc: 76.32\t train_loss: 0.5902\n",
      "train\t epoch:0/3\t batch_index:91\t train_acc: 76.4\t train_loss: 0.5893\n",
      "train\t epoch:0/3\t batch_index:92\t train_acc: 76.46\t train_loss: 0.5886\n",
      "train\t epoch:0/3\t batch_index:93\t train_acc: 76.59\t train_loss: 0.5876\n",
      "train\t epoch:0/3\t batch_index:94\t train_acc: 76.59\t train_loss: 0.5872\n",
      "train\t epoch:0/3\t batch_index:95\t train_acc: 76.64\t train_loss: 0.5866\n",
      "train\t epoch:0/3\t batch_index:96\t train_acc: 76.72\t train_loss: 0.5856\n",
      "train\t epoch:0/3\t batch_index:97\t train_acc: 76.75\t train_loss: 0.5853\n",
      "train\t epoch:0/3\t batch_index:98\t train_acc: 76.8\t train_loss: 0.5845\n",
      "train\t epoch:0/3\t batch_index:99\t train_acc: 76.91\t train_loss: 0.5837\n",
      "train\t epoch:0/3\t batch_index:100\t train_acc: 76.97\t train_loss: 0.5829\n",
      "train\t epoch:0/3\t batch_index:101\t train_acc: 77.07\t train_loss: 0.5818\n",
      "train\t epoch:0/3\t batch_index:102\t train_acc: 77.11\t train_loss: 0.5812\n",
      "train\t epoch:0/3\t batch_index:103\t train_acc: 77.17\t train_loss: 0.5805\n",
      "train\t epoch:0/3\t batch_index:104\t train_acc: 77.25\t train_loss: 0.5794\n",
      "train\t epoch:0/3\t batch_index:105\t train_acc: 77.33\t train_loss: 0.5785\n",
      "train\t epoch:0/3\t batch_index:106\t train_acc: 77.39\t train_loss: 0.5778\n",
      "train\t epoch:0/3\t batch_index:107\t train_acc: 77.49\t train_loss: 0.5772\n",
      "train\t epoch:0/3\t batch_index:108\t train_acc: 77.53\t train_loss: 0.5769\n",
      "train\t epoch:0/3\t batch_index:109\t train_acc: 77.6\t train_loss: 0.5761\n",
      "train\t epoch:0/3\t batch_index:110\t train_acc: 77.66\t train_loss: 0.5752\n",
      "train\t epoch:0/3\t batch_index:111\t train_acc: 77.72\t train_loss: 0.5744\n",
      "train\t epoch:0/3\t batch_index:112\t train_acc: 77.78\t train_loss: 0.5738\n",
      "train\t epoch:0/3\t batch_index:113\t train_acc: 77.85\t train_loss: 0.573\n",
      "train\t epoch:0/3\t batch_index:114\t train_acc: 77.97\t train_loss: 0.572\n",
      "train\t epoch:0/3\t batch_index:115\t train_acc: 78.03\t train_loss: 0.5711\n",
      "train\t epoch:0/3\t batch_index:116\t train_acc: 78.1\t train_loss: 0.5705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\t epoch:0/3\t batch_index:117\t train_acc: 78.15\t train_loss: 0.57\n",
      "train\t epoch:0/3\t batch_index:118\t train_acc: 78.23\t train_loss: 0.5694\n",
      "train\t epoch:0/3\t batch_index:119\t train_acc: 78.27\t train_loss: 0.5689\n",
      "train\t epoch:0/3\t batch_index:120\t train_acc: 78.34\t train_loss: 0.5681\n",
      "train\t epoch:0/3\t batch_index:121\t train_acc: 78.38\t train_loss: 0.5674\n",
      "train\t epoch:0/3\t batch_index:122\t train_acc: 78.46\t train_loss: 0.5666\n",
      "train\t epoch:0/3\t batch_index:123\t train_acc: 78.51\t train_loss: 0.5658\n",
      "train\t epoch:0/3\t batch_index:124\t train_acc: 78.54\t train_loss: 0.5653\n",
      "train\t epoch:0/3\t batch_index:125\t train_acc: 78.62\t train_loss: 0.5645\n",
      "train\t epoch:0/3\t batch_index:126\t train_acc: 78.7\t train_loss: 0.5639\n",
      "train\t epoch:0/3\t batch_index:127\t train_acc: 78.79\t train_loss: 0.5632\n",
      "train\t epoch:0/3\t batch_index:128\t train_acc: 78.82\t train_loss: 0.5628\n",
      "train\t epoch:0/3\t batch_index:129\t train_acc: 78.89\t train_loss: 0.5621\n",
      "train\t epoch:0/3\t batch_index:130\t train_acc: 78.97\t train_loss: 0.5612\n",
      "train\t epoch:0/3\t batch_index:131\t train_acc: 79.02\t train_loss: 0.5607\n",
      "train\t epoch:0/3\t batch_index:132\t train_acc: 79.07\t train_loss: 0.5599\n",
      "train\t epoch:0/3\t batch_index:133\t train_acc: 79.09\t train_loss: 0.5594\n",
      "train\t epoch:0/3\t batch_index:134\t train_acc: 79.11\t train_loss: 0.5588\n",
      "train\t epoch:0/3\t batch_index:135\t train_acc: 79.15\t train_loss: 0.5584\n",
      "train\t epoch:0/3\t batch_index:136\t train_acc: 79.19\t train_loss: 0.5577\n",
      "train\t epoch:0/3\t batch_index:137\t train_acc: 79.21\t train_loss: 0.5572\n",
      "train\t epoch:0/3\t batch_index:138\t train_acc: 79.28\t train_loss: 0.5565\n",
      "train\t epoch:0/3\t batch_index:139\t train_acc: 79.31\t train_loss: 0.5558\n",
      "train\t epoch:0/3\t batch_index:140\t train_acc: 79.33\t train_loss: 0.5553\n",
      "train\t epoch:0/3\t batch_index:141\t train_acc: 79.37\t train_loss: 0.5546\n",
      "train\t epoch:0/3\t batch_index:142\t train_acc: 79.41\t train_loss: 0.554\n",
      "train\t epoch:0/3\t batch_index:143\t train_acc: 79.47\t train_loss: 0.5535\n",
      "train\t epoch:0/3\t batch_index:144\t train_acc: 79.53\t train_loss: 0.5527\n",
      "train\t epoch:0/3\t batch_index:145\t train_acc: 79.56\t train_loss: 0.5523\n",
      "train\t epoch:0/3\t batch_index:146\t train_acc: 79.57\t train_loss: 0.5518\n",
      "train\t epoch:0/3\t batch_index:147\t train_acc: 79.6\t train_loss: 0.5513\n",
      "train\t epoch:0/3\t batch_index:148\t train_acc: 79.65\t train_loss: 0.5505\n",
      "train\t epoch:0/3\t batch_index:149\t train_acc: 79.69\t train_loss: 0.5499\n",
      "train\t epoch:0/3\t batch_index:150\t train_acc: 79.67\t train_loss: 0.5496\n",
      "train\t epoch:0/3\t batch_index:151\t train_acc: 79.73\t train_loss: 0.5489\n",
      "train\t epoch:0/3\t batch_index:152\t train_acc: 79.75\t train_loss: 0.5484\n",
      "train\t epoch:0/3\t batch_index:153\t train_acc: 79.77\t train_loss: 0.548\n",
      "train\t epoch:0/3\t batch_index:154\t train_acc: 79.82\t train_loss: 0.5473\n",
      "train\t epoch:0/3\t batch_index:155\t train_acc: 79.83\t train_loss: 0.5468\n",
      "train\t epoch:0/3\t batch_index:156\t train_acc: 79.86\t train_loss: 0.5461\n",
      "train\t epoch:0/3\t batch_index:157\t train_acc: 79.91\t train_loss: 0.5453\n",
      "train\t epoch:0/3\t batch_index:158\t train_acc: 79.96\t train_loss: 0.5447\n",
      "train\t epoch:0/3\t batch_index:159\t train_acc: 80.0\t train_loss: 0.5442\n",
      "train\t epoch:0/3\t batch_index:160\t train_acc: 79.99\t train_loss: 0.5439\n",
      "train\t epoch:0/3\t batch_index:161\t train_acc: 80.06\t train_loss: 0.5431\n",
      "train\t epoch:0/3\t batch_index:162\t train_acc: 80.09\t train_loss: 0.5427\n",
      "train\t epoch:0/3\t batch_index:163\t train_acc: 80.12\t train_loss: 0.5422\n",
      "train\t epoch:0/3\t batch_index:164\t train_acc: 80.13\t train_loss: 0.5418\n",
      "train\t epoch:0/3\t batch_index:165\t train_acc: 80.16\t train_loss: 0.5413\n",
      "train\t epoch:0/3\t batch_index:166\t train_acc: 80.22\t train_loss: 0.5405\n",
      "train\t epoch:0/3\t batch_index:167\t train_acc: 80.25\t train_loss: 0.5401\n",
      "train\t epoch:0/3\t batch_index:168\t train_acc: 80.29\t train_loss: 0.5394\n",
      "train\t epoch:0/3\t batch_index:169\t train_acc: 80.35\t train_loss: 0.5388\n",
      "train\t epoch:0/3\t batch_index:170\t train_acc: 80.38\t train_loss: 0.5384\n",
      "train\t epoch:0/3\t batch_index:171\t train_acc: 80.41\t train_loss: 0.5379\n",
      "train\t epoch:0/3\t batch_index:172\t train_acc: 80.41\t train_loss: 0.5375\n",
      "train\t epoch:0/3\t batch_index:173\t train_acc: 80.44\t train_loss: 0.5368\n",
      "train\t epoch:0/3\t batch_index:174\t train_acc: 80.48\t train_loss: 0.5363\n",
      "train\t epoch:0/3\t batch_index:175\t train_acc: 80.55\t train_loss: 0.5355\n",
      "train\t epoch:0/3\t batch_index:176\t train_acc: 80.58\t train_loss: 0.5352\n",
      "train\t epoch:0/3\t batch_index:177\t train_acc: 80.58\t train_loss: 0.535\n",
      "train\t epoch:0/3\t batch_index:178\t train_acc: 80.6\t train_loss: 0.5345\n",
      "train\t epoch:0/3\t batch_index:179\t train_acc: 80.62\t train_loss: 0.5341\n",
      "train\t epoch:0/3\t batch_index:180\t train_acc: 80.65\t train_loss: 0.5335\n",
      "train\t epoch:0/3\t batch_index:181\t train_acc: 80.69\t train_loss: 0.533\n",
      "train\t epoch:0/3\t batch_index:182\t train_acc: 80.7\t train_loss: 0.5326\n",
      "train\t epoch:0/3\t batch_index:183\t train_acc: 80.7\t train_loss: 0.5321\n",
      "train\t epoch:0/3\t batch_index:184\t train_acc: 80.72\t train_loss: 0.5316\n",
      "train\t epoch:0/3\t batch_index:185\t train_acc: 80.77\t train_loss: 0.5308\n",
      "train\t epoch:0/3\t batch_index:186\t train_acc: 80.79\t train_loss: 0.5302\n",
      "train\t epoch:0/3\t batch_index:187\t train_acc: 80.83\t train_loss: 0.5295\n",
      "train\t epoch:0/3\t batch_index:188\t train_acc: 80.89\t train_loss: 0.5289\n",
      "train\t epoch:0/3\t batch_index:189\t train_acc: 80.93\t train_loss: 0.5283\n",
      "train\t epoch:0/3\t batch_index:190\t train_acc: 80.96\t train_loss: 0.5278\n",
      "train\t epoch:0/3\t batch_index:191\t train_acc: 80.97\t train_loss: 0.5274\n",
      "train\t epoch:0/3\t batch_index:192\t train_acc: 80.99\t train_loss: 0.5269\n",
      "train\t epoch:0/3\t batch_index:193\t train_acc: 81.04\t train_loss: 0.5263\n",
      "train\t epoch:0/3\t batch_index:194\t train_acc: 81.05\t train_loss: 0.5259\n",
      "train\t epoch:0/3\t batch_index:195\t train_acc: 81.06\t train_loss: 0.5257\n",
      "train\t epoch:0/3\t batch_index:196\t train_acc: 81.08\t train_loss: 0.5254\n",
      "train\t epoch:0/3\t batch_index:197\t train_acc: 81.1\t train_loss: 0.5249\n",
      "train\t epoch:0/3\t batch_index:198\t train_acc: 81.09\t train_loss: 0.5248\n",
      "train\t epoch:0/3\t batch_index:199\t train_acc: 81.12\t train_loss: 0.5243\n",
      "train\t epoch:0/3\t batch_index:200\t train_acc: 81.12\t train_loss: 0.5239\n",
      "train\t epoch:0/3\t batch_index:201\t train_acc: 81.16\t train_loss: 0.5235\n",
      "train\t epoch:0/3\t batch_index:202\t train_acc: 81.19\t train_loss: 0.5231\n",
      "train\t epoch:0/3\t batch_index:203\t train_acc: 81.2\t train_loss: 0.5227\n",
      "train\t epoch:0/3\t batch_index:204\t train_acc: 81.24\t train_loss: 0.5221\n",
      "train\t epoch:0/3\t batch_index:205\t train_acc: 81.28\t train_loss: 0.5217\n",
      "train\t epoch:0/3\t batch_index:206\t train_acc: 81.31\t train_loss: 0.5212\n",
      "train\t epoch:0/3\t batch_index:207\t train_acc: 81.34\t train_loss: 0.5207\n",
      "train\t epoch:0/3\t batch_index:208\t train_acc: 81.36\t train_loss: 0.5202\n",
      "train\t epoch:0/3\t batch_index:209\t train_acc: 81.39\t train_loss: 0.5198\n",
      "train\t epoch:0/3\t batch_index:210\t train_acc: 81.41\t train_loss: 0.5195\n",
      "train\t epoch:0/3\t batch_index:211\t train_acc: 81.43\t train_loss: 0.5191\n",
      "train\t epoch:0/3\t batch_index:212\t train_acc: 81.44\t train_loss: 0.5185\n",
      "train\t epoch:0/3\t batch_index:213\t train_acc: 81.44\t train_loss: 0.5183\n",
      "train\t epoch:0/3\t batch_index:214\t train_acc: 81.48\t train_loss: 0.5179\n",
      "train\t epoch:0/3\t batch_index:215\t train_acc: 81.49\t train_loss: 0.5175\n",
      "train\t epoch:0/3\t batch_index:216\t train_acc: 81.53\t train_loss: 0.5169\n",
      "train\t epoch:0/3\t batch_index:217\t train_acc: 81.55\t train_loss: 0.5166\n",
      "train\t epoch:0/3\t batch_index:218\t train_acc: 81.6\t train_loss: 0.5161\n",
      "train\t epoch:0/3\t batch_index:219\t train_acc: 81.62\t train_loss: 0.5158\n",
      "train\t epoch:0/3\t batch_index:220\t train_acc: 81.66\t train_loss: 0.5153\n",
      "train\t epoch:0/3\t batch_index:221\t train_acc: 81.69\t train_loss: 0.5148\n",
      "train\t epoch:0/3\t batch_index:222\t train_acc: 81.73\t train_loss: 0.5143\n",
      "train\t epoch:0/3\t batch_index:223\t train_acc: 81.75\t train_loss: 0.5138\n",
      "train\t epoch:0/3\t batch_index:224\t train_acc: 81.77\t train_loss: 0.5136\n",
      "train\t epoch:0/3\t batch_index:225\t train_acc: 81.79\t train_loss: 0.5131\n",
      "train\t epoch:0/3\t batch_index:226\t train_acc: 81.81\t train_loss: 0.5127\n",
      "train\t epoch:0/3\t batch_index:227\t train_acc: 81.83\t train_loss: 0.5123\n",
      "train\t epoch:0/3\t batch_index:228\t train_acc: 81.86\t train_loss: 0.512\n",
      "train\t epoch:0/3\t batch_index:229\t train_acc: 81.87\t train_loss: 0.5116\n",
      "train\t epoch:0/3\t batch_index:230\t train_acc: 81.91\t train_loss: 0.5111\n",
      "train\t epoch:0/3\t batch_index:231\t train_acc: 81.94\t train_loss: 0.5105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\t epoch:0/3\t batch_index:232\t train_acc: 81.98\t train_loss: 0.5101\n",
      "train\t epoch:0/3\t batch_index:233\t train_acc: 82.0\t train_loss: 0.5095\n",
      "train\t epoch:0/3\t batch_index:234\t train_acc: 82.01\t train_loss: 0.5091\n",
      "train\t epoch:0/3\t batch_index:235\t train_acc: 82.02\t train_loss: 0.5088\n",
      "train\t epoch:0/3\t batch_index:236\t train_acc: 82.04\t train_loss: 0.5082\n",
      "train\t epoch:0/3\t batch_index:237\t train_acc: 82.07\t train_loss: 0.5078\n",
      "train\t epoch:0/3\t batch_index:238\t train_acc: 82.09\t train_loss: 0.5075\n",
      "train\t epoch:0/3\t batch_index:239\t train_acc: 82.12\t train_loss: 0.5071\n",
      "train\t epoch:0/3\t batch_index:240\t train_acc: 82.13\t train_loss: 0.5067\n",
      "train\t epoch:0/3\t batch_index:241\t train_acc: 82.15\t train_loss: 0.5063\n",
      "train\t epoch:0/3\t batch_index:242\t train_acc: 82.17\t train_loss: 0.506\n",
      "train\t epoch:0/3\t batch_index:243\t train_acc: 82.18\t train_loss: 0.5057\n",
      "train\t epoch:0/3\t batch_index:244\t train_acc: 82.2\t train_loss: 0.5052\n",
      "train\t epoch:0/3\t batch_index:245\t train_acc: 82.22\t train_loss: 0.5048\n",
      "train\t epoch:0/3\t batch_index:246\t train_acc: 82.27\t train_loss: 0.504\n",
      "train\t epoch:0/3\t batch_index:247\t train_acc: 82.3\t train_loss: 0.5036\n",
      "train\t epoch:0/3\t batch_index:248\t train_acc: 82.31\t train_loss: 0.5032\n",
      "train\t epoch:0/3\t batch_index:249\t train_acc: 82.35\t train_loss: 0.5026\n",
      "train\t epoch:0/3\t batch_index:250\t train_acc: 82.37\t train_loss: 0.5022\n",
      "train\t epoch:0/3\t batch_index:251\t train_acc: 82.39\t train_loss: 0.5019\n",
      "train\t epoch:0/3\t batch_index:252\t train_acc: 82.4\t train_loss: 0.5014\n",
      "train\t epoch:0/3\t batch_index:253\t train_acc: 82.43\t train_loss: 0.5009\n",
      "train\t epoch:0/3\t batch_index:254\t train_acc: 82.44\t train_loss: 0.5006\n",
      "train\t epoch:0/3\t batch_index:255\t train_acc: 82.46\t train_loss: 0.5003\n",
      "train\t epoch:0/3\t batch_index:256\t train_acc: 82.49\t train_loss: 0.4998\n",
      "train\t epoch:0/3\t batch_index:257\t train_acc: 82.52\t train_loss: 0.4994\n",
      "train\t epoch:0/3\t batch_index:258\t train_acc: 82.55\t train_loss: 0.4989\n",
      "train\t epoch:0/3\t batch_index:259\t train_acc: 82.57\t train_loss: 0.4985\n",
      "train\t epoch:0/3\t batch_index:260\t train_acc: 82.6\t train_loss: 0.4981\n",
      "train\t epoch:0/3\t batch_index:261\t train_acc: 82.63\t train_loss: 0.4977\n",
      "train\t epoch:0/3\t batch_index:262\t train_acc: 82.64\t train_loss: 0.4974\n",
      "train\t epoch:0/3\t batch_index:263\t train_acc: 82.66\t train_loss: 0.497\n",
      "train\t epoch:0/3\t batch_index:264\t train_acc: 82.69\t train_loss: 0.4966\n",
      "train\t epoch:0/3\t batch_index:265\t train_acc: 82.7\t train_loss: 0.4962\n",
      "train\t epoch:0/3\t batch_index:266\t train_acc: 82.72\t train_loss: 0.4959\n",
      "train\t epoch:0/3\t batch_index:267\t train_acc: 82.73\t train_loss: 0.4954\n",
      "train\t epoch:0/3\t batch_index:268\t train_acc: 82.75\t train_loss: 0.4951\n",
      "train\t epoch:0/3\t batch_index:269\t train_acc: 82.76\t train_loss: 0.4948\n",
      "train\t epoch:0/3\t batch_index:270\t train_acc: 82.78\t train_loss: 0.4944\n",
      "train\t epoch:0/3\t batch_index:271\t train_acc: 82.8\t train_loss: 0.4942\n",
      "train\t epoch:0/3\t batch_index:272\t train_acc: 82.81\t train_loss: 0.4938\n",
      "train\t epoch:0/3\t batch_index:273\t train_acc: 82.83\t train_loss: 0.4935\n",
      "train\t epoch:0/3\t batch_index:274\t train_acc: 82.85\t train_loss: 0.4931\n",
      "train\t epoch:0/3\t batch_index:275\t train_acc: 82.87\t train_loss: 0.4926\n",
      "train\t epoch:0/3\t batch_index:276\t train_acc: 82.9\t train_loss: 0.4922\n",
      "train\t epoch:0/3\t batch_index:277\t train_acc: 82.92\t train_loss: 0.4918\n",
      "train\t epoch:0/3\t batch_index:278\t train_acc: 82.91\t train_loss: 0.4916\n",
      "train\t epoch:0/3\t batch_index:279\t train_acc: 82.93\t train_loss: 0.4911\n",
      "train\t epoch:0/3\t batch_index:280\t train_acc: 82.95\t train_loss: 0.4907\n",
      "train\t epoch:0/3\t batch_index:281\t train_acc: 82.96\t train_loss: 0.4905\n",
      "train\t epoch:0/3\t batch_index:282\t train_acc: 82.99\t train_loss: 0.4901\n",
      "train\t epoch:0/3\t batch_index:283\t train_acc: 83.01\t train_loss: 0.4897\n",
      "train\t epoch:0/3\t batch_index:284\t train_acc: 83.0\t train_loss: 0.4895\n",
      "train\t epoch:0/3\t batch_index:285\t train_acc: 83.02\t train_loss: 0.4892\n",
      "train\t epoch:0/3\t batch_index:286\t train_acc: 83.02\t train_loss: 0.4889\n",
      "train\t epoch:0/3\t batch_index:287\t train_acc: 83.06\t train_loss: 0.4884\n",
      "train\t epoch:0/3\t batch_index:288\t train_acc: 83.07\t train_loss: 0.4881\n",
      "train\t epoch:0/3\t batch_index:289\t train_acc: 83.1\t train_loss: 0.4877\n",
      "train\t epoch:0/3\t batch_index:290\t train_acc: 83.11\t train_loss: 0.4874\n",
      "train\t epoch:0/3\t batch_index:291\t train_acc: 83.12\t train_loss: 0.487\n",
      "train\t epoch:0/3\t batch_index:292\t train_acc: 83.14\t train_loss: 0.4867\n",
      "train\t epoch:0/3\t batch_index:293\t train_acc: 83.14\t train_loss: 0.4864\n",
      "train\t epoch:0/3\t batch_index:294\t train_acc: 83.16\t train_loss: 0.486\n",
      "train\t epoch:0/3\t batch_index:295\t train_acc: 83.19\t train_loss: 0.4856\n",
      "train\t epoch:0/3\t batch_index:296\t train_acc: 83.21\t train_loss: 0.4852\n",
      "train\t epoch:0/3\t batch_index:297\t train_acc: 83.2\t train_loss: 0.4851\n",
      "train\t epoch:0/3\t batch_index:298\t train_acc: 83.23\t train_loss: 0.4846\n",
      "train\t epoch:0/3\t batch_index:299\t train_acc: 83.25\t train_loss: 0.4842\n",
      "train\t epoch:0/3\t batch_index:300\t train_acc: 83.27\t train_loss: 0.4838\n",
      "train\t epoch:0/3\t batch_index:301\t train_acc: 83.28\t train_loss: 0.4835\n",
      "train\t epoch:0/3\t batch_index:302\t train_acc: 83.29\t train_loss: 0.4833\n",
      "train\t epoch:0/3\t batch_index:303\t train_acc: 83.31\t train_loss: 0.4829\n",
      "train\t epoch:0/3\t batch_index:304\t train_acc: 83.33\t train_loss: 0.4826\n",
      "train\t epoch:0/3\t batch_index:305\t train_acc: 83.35\t train_loss: 0.4822\n",
      "val\t epoch:0/3\t batch_index:0\t val_acc: 90.62\t val_loss: 0.3936\n",
      "val\t epoch:0/3\t batch_index:1\t val_acc: 91.8\t val_loss: 0.3856\n",
      "val\t epoch:0/3\t batch_index:2\t val_acc: 91.41\t val_loss: 0.3817\n",
      "val\t epoch:0/3\t batch_index:3\t val_acc: 89.84\t val_loss: 0.3889\n",
      "val\t epoch:0/3\t batch_index:4\t val_acc: 89.69\t val_loss: 0.3773\n",
      "val\t epoch:0/3\t batch_index:5\t val_acc: 89.19\t val_loss: 0.3806\n",
      "val\t epoch:0/3\t batch_index:6\t val_acc: 89.06\t val_loss: 0.3829\n",
      "val\t epoch:0/3\t batch_index:7\t val_acc: 89.06\t val_loss: 0.3809\n",
      "val\t epoch:0/3\t batch_index:8\t val_acc: 88.8\t val_loss: 0.3885\n",
      "val\t epoch:0/3\t batch_index:9\t val_acc: 88.91\t val_loss: 0.3861\n",
      "val\t epoch:0/3\t batch_index:10\t val_acc: 88.64\t val_loss: 0.387\n",
      "val\t epoch:0/3\t batch_index:11\t val_acc: 88.41\t val_loss: 0.3892\n",
      "val\t epoch:0/3\t batch_index:12\t val_acc: 88.52\t val_loss: 0.3871\n",
      "val\t epoch:0/3\t batch_index:13\t val_acc: 88.34\t val_loss: 0.3892\n",
      "val\t epoch:0/3\t batch_index:14\t val_acc: 88.54\t val_loss: 0.3858\n",
      "val\t epoch:0/3\t batch_index:15\t val_acc: 88.38\t val_loss: 0.3885\n",
      "val\t epoch:0/3\t batch_index:16\t val_acc: 88.42\t val_loss: 0.3864\n",
      "val\t epoch:0/3\t batch_index:17\t val_acc: 88.5\t val_loss: 0.3863\n",
      "val\t epoch:0/3\t batch_index:18\t val_acc: 88.57\t val_loss: 0.3854\n",
      "val\t epoch:0/3\t batch_index:19\t val_acc: 88.48\t val_loss: 0.3868\n",
      "val\t epoch:0/3\t batch_index:20\t val_acc: 88.47\t val_loss: 0.3865\n",
      "val\t epoch:0/3\t batch_index:21\t val_acc: 88.32\t val_loss: 0.3869\n",
      "val\t epoch:0/3\t batch_index:22\t val_acc: 88.25\t val_loss: 0.3886\n",
      "val\t epoch:0/3\t batch_index:23\t val_acc: 88.15\t val_loss: 0.3889\n",
      "val\t epoch:0/3\t batch_index:24\t val_acc: 88.16\t val_loss: 0.3886\n",
      "val\t epoch:0/3\t batch_index:25\t val_acc: 88.22\t val_loss: 0.3875\n",
      "val\t epoch:0/3\t batch_index:26\t val_acc: 88.43\t val_loss: 0.386\n",
      "val\t epoch:0/3\t batch_index:27\t val_acc: 88.42\t val_loss: 0.3857\n",
      "val\t epoch:0/3\t batch_index:28\t val_acc: 88.28\t val_loss: 0.3873\n",
      "val\t epoch:0/3\t batch_index:29\t val_acc: 88.28\t val_loss: 0.3866\n",
      "val\t epoch:0/3\t batch_index:30\t val_acc: 88.26\t val_loss: 0.3868\n",
      "val\t epoch:0/3\t batch_index:31\t val_acc: 88.26\t val_loss: 0.3868\n",
      "val\t epoch:0/3\t batch_index:32\t val_acc: 88.33\t val_loss: 0.386\n",
      "val\t epoch:0/3\t batch_index:33\t val_acc: 88.24\t val_loss: 0.3863\n",
      "val\t epoch:0/3\t batch_index:34\t val_acc: 88.26\t val_loss: 0.3861\n",
      "val\t epoch:0/3\t batch_index:35\t val_acc: 88.41\t val_loss: 0.3849\n",
      "val\t epoch:0/3\t batch_index:36\t val_acc: 88.45\t val_loss: 0.3846\n",
      "val\t epoch:0/3\t batch_index:37\t val_acc: 88.4\t val_loss: 0.3845\n",
      "val\t epoch:0/3\t batch_index:38\t val_acc: 88.3\t val_loss: 0.3848\n",
      "val\t epoch:0/3\t batch_index:39\t val_acc: 88.34\t val_loss: 0.3848\n",
      "val\t epoch:0/3\t batch_index:40\t val_acc: 88.36\t val_loss: 0.3846\n",
      "val\t epoch:0/3\t batch_index:41\t val_acc: 88.43\t val_loss: 0.384\n",
      "val\t epoch:0/3\t batch_index:42\t val_acc: 88.26\t val_loss: 0.385\n",
      "val\t epoch:0/3\t batch_index:43\t val_acc: 88.28\t val_loss: 0.3852\n",
      "val\t epoch:0/3\t batch_index:44\t val_acc: 88.35\t val_loss: 0.3846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val\t epoch:0/3\t batch_index:45\t val_acc: 88.4\t val_loss: 0.3844\n",
      "val\t epoch:0/3\t batch_index:46\t val_acc: 88.35\t val_loss: 0.3845\n",
      "val\t epoch:0/3\t batch_index:47\t val_acc: 88.31\t val_loss: 0.3848\n",
      "val\t epoch:0/3\t batch_index:48\t val_acc: 88.33\t val_loss: 0.3845\n",
      "val\t epoch:0/3\t batch_index:49\t val_acc: 88.3\t val_loss: 0.3844\n",
      "val\t epoch:0/3\t batch_index:50\t val_acc: 88.27\t val_loss: 0.3843\n",
      "val\t epoch:0/3\t batch_index:51\t val_acc: 88.16\t val_loss: 0.3854\n",
      "val\t epoch:0/3\t batch_index:52\t val_acc: 88.18\t val_loss: 0.3857\n",
      "val\t epoch:0/3\t batch_index:53\t val_acc: 88.18\t val_loss: 0.3859\n",
      "val\t epoch:0/3\t batch_index:54\t val_acc: 88.21\t val_loss: 0.3858\n",
      "val\t epoch:0/3\t batch_index:55\t val_acc: 88.23\t val_loss: 0.3849\n",
      "val\t epoch:0/3\t batch_index:56\t val_acc: 88.25\t val_loss: 0.3844\n",
      "val\t epoch:0/3\t batch_index:57\t val_acc: 88.23\t val_loss: 0.3843\n",
      "val\t epoch:0/3\t batch_index:58\t val_acc: 88.19\t val_loss: 0.3848\n",
      "val\t epoch:0/3\t batch_index:59\t val_acc: 88.14\t val_loss: 0.3853\n",
      "val\t epoch:0/3\t batch_index:60\t val_acc: 88.19\t val_loss: 0.3847\n",
      "val\t epoch:0/3\t batch_index:61\t val_acc: 88.27\t val_loss: 0.3837\n",
      "val\t epoch:0/3\t batch_index:62\t val_acc: 88.24\t val_loss: 0.3842\n",
      "val\t epoch:0/3\t batch_index:63\t val_acc: 88.29\t val_loss: 0.3841\n",
      "val\t epoch:0/3\t batch_index:64\t val_acc: 88.27\t val_loss: 0.3845\n",
      "train\t epoch:1/3\t batch_index:0\t train_acc: 95.31\t train_loss: 0.3212\n",
      "train\t epoch:1/3\t batch_index:1\t train_acc: 88.67\t train_loss: 0.3737\n",
      "train\t epoch:1/3\t batch_index:2\t train_acc: 88.28\t train_loss: 0.3781\n",
      "train\t epoch:1/3\t batch_index:3\t train_acc: 88.87\t train_loss: 0.3721\n",
      "train\t epoch:1/3\t batch_index:4\t train_acc: 89.38\t train_loss: 0.3662\n",
      "train\t epoch:1/3\t batch_index:5\t train_acc: 89.06\t train_loss: 0.3736\n",
      "train\t epoch:1/3\t batch_index:6\t train_acc: 89.29\t train_loss: 0.3705\n",
      "train\t epoch:1/3\t batch_index:7\t train_acc: 89.75\t train_loss: 0.3686\n",
      "train\t epoch:1/3\t batch_index:8\t train_acc: 89.5\t train_loss: 0.3677\n",
      "train\t epoch:1/3\t batch_index:9\t train_acc: 89.69\t train_loss: 0.3656\n",
      "train\t epoch:1/3\t batch_index:10\t train_acc: 89.91\t train_loss: 0.3625\n",
      "train\t epoch:1/3\t batch_index:11\t train_acc: 89.84\t train_loss: 0.3644\n",
      "train\t epoch:1/3\t batch_index:12\t train_acc: 89.66\t train_loss: 0.3648\n",
      "train\t epoch:1/3\t batch_index:13\t train_acc: 89.62\t train_loss: 0.3631\n",
      "train\t epoch:1/3\t batch_index:14\t train_acc: 89.53\t train_loss: 0.3638\n",
      "train\t epoch:1/3\t batch_index:15\t train_acc: 89.4\t train_loss: 0.3618\n",
      "train\t epoch:1/3\t batch_index:16\t train_acc: 89.2\t train_loss: 0.3632\n",
      "train\t epoch:1/3\t batch_index:17\t train_acc: 89.28\t train_loss: 0.3625\n",
      "train\t epoch:1/3\t batch_index:18\t train_acc: 89.31\t train_loss: 0.3632\n",
      "train\t epoch:1/3\t batch_index:19\t train_acc: 89.41\t train_loss: 0.3628\n",
      "train\t epoch:1/3\t batch_index:20\t train_acc: 89.4\t train_loss: 0.3623\n",
      "train\t epoch:1/3\t batch_index:21\t train_acc: 89.49\t train_loss: 0.3609\n",
      "train\t epoch:1/3\t batch_index:22\t train_acc: 89.37\t train_loss: 0.363\n",
      "train\t epoch:1/3\t batch_index:23\t train_acc: 89.49\t train_loss: 0.3623\n",
      "train\t epoch:1/3\t batch_index:24\t train_acc: 89.56\t train_loss: 0.3616\n",
      "train\t epoch:1/3\t batch_index:25\t train_acc: 89.6\t train_loss: 0.3613\n",
      "train\t epoch:1/3\t batch_index:26\t train_acc: 89.67\t train_loss: 0.3615\n",
      "train\t epoch:1/3\t batch_index:27\t train_acc: 89.7\t train_loss: 0.3603\n",
      "train\t epoch:1/3\t batch_index:28\t train_acc: 89.74\t train_loss: 0.36\n",
      "train\t epoch:1/3\t batch_index:29\t train_acc: 89.58\t train_loss: 0.3616\n",
      "train\t epoch:1/3\t batch_index:30\t train_acc: 89.57\t train_loss: 0.3619\n",
      "train\t epoch:1/3\t batch_index:31\t train_acc: 89.6\t train_loss: 0.3621\n",
      "train\t epoch:1/3\t batch_index:32\t train_acc: 89.63\t train_loss: 0.3618\n",
      "train\t epoch:1/3\t batch_index:33\t train_acc: 89.57\t train_loss: 0.3633\n",
      "train\t epoch:1/3\t batch_index:34\t train_acc: 89.6\t train_loss: 0.3628\n",
      "train\t epoch:1/3\t batch_index:35\t train_acc: 89.61\t train_loss: 0.3627\n",
      "train\t epoch:1/3\t batch_index:36\t train_acc: 89.55\t train_loss: 0.3622\n",
      "train\t epoch:1/3\t batch_index:37\t train_acc: 89.51\t train_loss: 0.3626\n",
      "train\t epoch:1/3\t batch_index:38\t train_acc: 89.4\t train_loss: 0.3635\n",
      "train\t epoch:1/3\t batch_index:39\t train_acc: 89.43\t train_loss: 0.3629\n",
      "train\t epoch:1/3\t batch_index:40\t train_acc: 89.5\t train_loss: 0.3621\n",
      "train\t epoch:1/3\t batch_index:41\t train_acc: 89.45\t train_loss: 0.3625\n",
      "train\t epoch:1/3\t batch_index:42\t train_acc: 89.59\t train_loss: 0.3619\n",
      "train\t epoch:1/3\t batch_index:43\t train_acc: 89.52\t train_loss: 0.3618\n",
      "train\t epoch:1/3\t batch_index:44\t train_acc: 89.5\t train_loss: 0.3621\n",
      "train\t epoch:1/3\t batch_index:45\t train_acc: 89.45\t train_loss: 0.3621\n",
      "train\t epoch:1/3\t batch_index:46\t train_acc: 89.46\t train_loss: 0.3614\n",
      "train\t epoch:1/3\t batch_index:47\t train_acc: 89.45\t train_loss: 0.3614\n",
      "train\t epoch:1/3\t batch_index:48\t train_acc: 89.46\t train_loss: 0.3619\n",
      "train\t epoch:1/3\t batch_index:49\t train_acc: 89.45\t train_loss: 0.3612\n",
      "train\t epoch:1/3\t batch_index:50\t train_acc: 89.35\t train_loss: 0.3615\n",
      "train\t epoch:1/3\t batch_index:51\t train_acc: 89.36\t train_loss: 0.3613\n",
      "train\t epoch:1/3\t batch_index:52\t train_acc: 89.33\t train_loss: 0.3613\n",
      "train\t epoch:1/3\t batch_index:53\t train_acc: 89.32\t train_loss: 0.3613\n",
      "train\t epoch:1/3\t batch_index:54\t train_acc: 89.3\t train_loss: 0.3611\n",
      "train\t epoch:1/3\t batch_index:55\t train_acc: 89.22\t train_loss: 0.3614\n",
      "train\t epoch:1/3\t batch_index:56\t train_acc: 89.28\t train_loss: 0.3605\n",
      "train\t epoch:1/3\t batch_index:57\t train_acc: 89.28\t train_loss: 0.3603\n",
      "train\t epoch:1/3\t batch_index:58\t train_acc: 89.29\t train_loss: 0.3601\n",
      "train\t epoch:1/3\t batch_index:59\t train_acc: 89.32\t train_loss: 0.36\n",
      "train\t epoch:1/3\t batch_index:60\t train_acc: 89.23\t train_loss: 0.3604\n",
      "train\t epoch:1/3\t batch_index:61\t train_acc: 89.23\t train_loss: 0.36\n",
      "train\t epoch:1/3\t batch_index:62\t train_acc: 89.29\t train_loss: 0.3596\n",
      "train\t epoch:1/3\t batch_index:63\t train_acc: 89.29\t train_loss: 0.3594\n",
      "train\t epoch:1/3\t batch_index:64\t train_acc: 89.29\t train_loss: 0.3594\n",
      "train\t epoch:1/3\t batch_index:65\t train_acc: 89.28\t train_loss: 0.3591\n",
      "train\t epoch:1/3\t batch_index:66\t train_acc: 89.26\t train_loss: 0.3598\n",
      "train\t epoch:1/3\t batch_index:67\t train_acc: 89.2\t train_loss: 0.3597\n",
      "train\t epoch:1/3\t batch_index:68\t train_acc: 89.14\t train_loss: 0.3598\n",
      "train\t epoch:1/3\t batch_index:69\t train_acc: 89.23\t train_loss: 0.3591\n",
      "train\t epoch:1/3\t batch_index:70\t train_acc: 89.19\t train_loss: 0.359\n",
      "train\t epoch:1/3\t batch_index:71\t train_acc: 89.25\t train_loss: 0.3587\n",
      "train\t epoch:1/3\t batch_index:72\t train_acc: 89.24\t train_loss: 0.3588\n",
      "train\t epoch:1/3\t batch_index:73\t train_acc: 89.27\t train_loss: 0.359\n",
      "train\t epoch:1/3\t batch_index:74\t train_acc: 89.29\t train_loss: 0.3589\n",
      "train\t epoch:1/3\t batch_index:75\t train_acc: 89.29\t train_loss: 0.3592\n",
      "train\t epoch:1/3\t batch_index:76\t train_acc: 89.27\t train_loss: 0.3595\n",
      "train\t epoch:1/3\t batch_index:77\t train_acc: 89.29\t train_loss: 0.3591\n",
      "train\t epoch:1/3\t batch_index:78\t train_acc: 89.35\t train_loss: 0.3587\n",
      "train\t epoch:1/3\t batch_index:79\t train_acc: 89.37\t train_loss: 0.3583\n",
      "train\t epoch:1/3\t batch_index:80\t train_acc: 89.34\t train_loss: 0.3586\n",
      "train\t epoch:1/3\t batch_index:81\t train_acc: 89.36\t train_loss: 0.3581\n",
      "train\t epoch:1/3\t batch_index:82\t train_acc: 89.35\t train_loss: 0.3582\n",
      "train\t epoch:1/3\t batch_index:83\t train_acc: 89.4\t train_loss: 0.3576\n",
      "train\t epoch:1/3\t batch_index:84\t train_acc: 89.36\t train_loss: 0.3576\n",
      "train\t epoch:1/3\t batch_index:85\t train_acc: 89.33\t train_loss: 0.3577\n",
      "train\t epoch:1/3\t batch_index:86\t train_acc: 89.34\t train_loss: 0.3575\n",
      "train\t epoch:1/3\t batch_index:87\t train_acc: 89.36\t train_loss: 0.3578\n",
      "train\t epoch:1/3\t batch_index:88\t train_acc: 89.38\t train_loss: 0.3579\n",
      "train\t epoch:1/3\t batch_index:89\t train_acc: 89.38\t train_loss: 0.3578\n",
      "train\t epoch:1/3\t batch_index:90\t train_acc: 89.42\t train_loss: 0.3573\n",
      "train\t epoch:1/3\t batch_index:91\t train_acc: 89.41\t train_loss: 0.3571\n",
      "train\t epoch:1/3\t batch_index:92\t train_acc: 89.43\t train_loss: 0.3567\n",
      "train\t epoch:1/3\t batch_index:93\t train_acc: 89.45\t train_loss: 0.3563\n",
      "train\t epoch:1/3\t batch_index:94\t train_acc: 89.52\t train_loss: 0.3557\n",
      "train\t epoch:1/3\t batch_index:95\t train_acc: 89.53\t train_loss: 0.3557\n",
      "train\t epoch:1/3\t batch_index:96\t train_acc: 89.55\t train_loss: 0.3557\n",
      "train\t epoch:1/3\t batch_index:97\t train_acc: 89.57\t train_loss: 0.3554\n",
      "train\t epoch:1/3\t batch_index:98\t train_acc: 89.56\t train_loss: 0.3555\n",
      "train\t epoch:1/3\t batch_index:99\t train_acc: 89.57\t train_loss: 0.3559\n",
      "train\t epoch:1/3\t batch_index:100\t train_acc: 89.6\t train_loss: 0.3554\n",
      "train\t epoch:1/3\t batch_index:101\t train_acc: 89.51\t train_loss: 0.3557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\t epoch:1/3\t batch_index:102\t train_acc: 89.49\t train_loss: 0.3557\n",
      "train\t epoch:1/3\t batch_index:103\t train_acc: 89.5\t train_loss: 0.3559\n",
      "train\t epoch:1/3\t batch_index:104\t train_acc: 89.52\t train_loss: 0.3563\n",
      "train\t epoch:1/3\t batch_index:105\t train_acc: 89.53\t train_loss: 0.3562\n",
      "train\t epoch:1/3\t batch_index:106\t train_acc: 89.49\t train_loss: 0.3564\n",
      "train\t epoch:1/3\t batch_index:107\t train_acc: 89.5\t train_loss: 0.3563\n",
      "train\t epoch:1/3\t batch_index:108\t train_acc: 89.47\t train_loss: 0.3562\n",
      "train\t epoch:1/3\t batch_index:109\t train_acc: 89.45\t train_loss: 0.3564\n",
      "train\t epoch:1/3\t batch_index:110\t train_acc: 89.42\t train_loss: 0.3564\n",
      "train\t epoch:1/3\t batch_index:111\t train_acc: 89.44\t train_loss: 0.3559\n",
      "train\t epoch:1/3\t batch_index:112\t train_acc: 89.48\t train_loss: 0.3555\n",
      "train\t epoch:1/3\t batch_index:113\t train_acc: 89.47\t train_loss: 0.3555\n",
      "train\t epoch:1/3\t batch_index:114\t train_acc: 89.47\t train_loss: 0.3555\n",
      "train\t epoch:1/3\t batch_index:115\t train_acc: 89.48\t train_loss: 0.3557\n",
      "train\t epoch:1/3\t batch_index:116\t train_acc: 89.5\t train_loss: 0.3556\n",
      "train\t epoch:1/3\t batch_index:117\t train_acc: 89.51\t train_loss: 0.3552\n",
      "train\t epoch:1/3\t batch_index:118\t train_acc: 89.52\t train_loss: 0.3549\n",
      "train\t epoch:1/3\t batch_index:119\t train_acc: 89.54\t train_loss: 0.3546\n",
      "train\t epoch:1/3\t batch_index:120\t train_acc: 89.55\t train_loss: 0.3545\n",
      "train\t epoch:1/3\t batch_index:121\t train_acc: 89.57\t train_loss: 0.354\n",
      "train\t epoch:1/3\t batch_index:122\t train_acc: 89.58\t train_loss: 0.3535\n",
      "train\t epoch:1/3\t batch_index:123\t train_acc: 89.59\t train_loss: 0.3534\n",
      "train\t epoch:1/3\t batch_index:124\t train_acc: 89.57\t train_loss: 0.3538\n",
      "train\t epoch:1/3\t batch_index:125\t train_acc: 89.56\t train_loss: 0.3536\n",
      "train\t epoch:1/3\t batch_index:126\t train_acc: 89.57\t train_loss: 0.3533\n",
      "train\t epoch:1/3\t batch_index:127\t train_acc: 89.56\t train_loss: 0.3534\n",
      "train\t epoch:1/3\t batch_index:128\t train_acc: 89.6\t train_loss: 0.3529\n",
      "train\t epoch:1/3\t batch_index:129\t train_acc: 89.59\t train_loss: 0.3529\n",
      "train\t epoch:1/3\t batch_index:130\t train_acc: 89.59\t train_loss: 0.3531\n",
      "train\t epoch:1/3\t batch_index:131\t train_acc: 89.61\t train_loss: 0.3529\n",
      "train\t epoch:1/3\t batch_index:132\t train_acc: 89.64\t train_loss: 0.3526\n",
      "train\t epoch:1/3\t batch_index:133\t train_acc: 89.65\t train_loss: 0.3522\n",
      "train\t epoch:1/3\t batch_index:134\t train_acc: 89.65\t train_loss: 0.3521\n",
      "train\t epoch:1/3\t batch_index:135\t train_acc: 89.65\t train_loss: 0.3521\n",
      "train\t epoch:1/3\t batch_index:136\t train_acc: 89.68\t train_loss: 0.3519\n",
      "train\t epoch:1/3\t batch_index:137\t train_acc: 89.67\t train_loss: 0.3518\n",
      "train\t epoch:1/3\t batch_index:138\t train_acc: 89.68\t train_loss: 0.3516\n",
      "train\t epoch:1/3\t batch_index:139\t train_acc: 89.69\t train_loss: 0.3516\n",
      "train\t epoch:1/3\t batch_index:140\t train_acc: 89.69\t train_loss: 0.3513\n",
      "train\t epoch:1/3\t batch_index:141\t train_acc: 89.69\t train_loss: 0.3512\n",
      "train\t epoch:1/3\t batch_index:142\t train_acc: 89.69\t train_loss: 0.351\n",
      "train\t epoch:1/3\t batch_index:143\t train_acc: 89.7\t train_loss: 0.3508\n",
      "train\t epoch:1/3\t batch_index:144\t train_acc: 89.69\t train_loss: 0.3509\n",
      "train\t epoch:1/3\t batch_index:145\t train_acc: 89.69\t train_loss: 0.3509\n",
      "train\t epoch:1/3\t batch_index:146\t train_acc: 89.68\t train_loss: 0.3509\n",
      "train\t epoch:1/3\t batch_index:147\t train_acc: 89.71\t train_loss: 0.3507\n",
      "train\t epoch:1/3\t batch_index:148\t train_acc: 89.71\t train_loss: 0.3506\n",
      "train\t epoch:1/3\t batch_index:149\t train_acc: 89.71\t train_loss: 0.3505\n",
      "train\t epoch:1/3\t batch_index:150\t train_acc: 89.69\t train_loss: 0.3505\n",
      "train\t epoch:1/3\t batch_index:151\t train_acc: 89.7\t train_loss: 0.3501\n",
      "train\t epoch:1/3\t batch_index:152\t train_acc: 89.71\t train_loss: 0.35\n",
      "train\t epoch:1/3\t batch_index:153\t train_acc: 89.68\t train_loss: 0.35\n",
      "train\t epoch:1/3\t batch_index:154\t train_acc: 89.67\t train_loss: 0.3499\n",
      "train\t epoch:1/3\t batch_index:155\t train_acc: 89.68\t train_loss: 0.3495\n",
      "train\t epoch:1/3\t batch_index:156\t train_acc: 89.69\t train_loss: 0.3494\n",
      "train\t epoch:1/3\t batch_index:157\t train_acc: 89.68\t train_loss: 0.3493\n",
      "train\t epoch:1/3\t batch_index:158\t train_acc: 89.7\t train_loss: 0.3491\n",
      "train\t epoch:1/3\t batch_index:159\t train_acc: 89.71\t train_loss: 0.3489\n",
      "train\t epoch:1/3\t batch_index:160\t train_acc: 89.73\t train_loss: 0.3487\n",
      "train\t epoch:1/3\t batch_index:161\t train_acc: 89.76\t train_loss: 0.3483\n",
      "train\t epoch:1/3\t batch_index:162\t train_acc: 89.78\t train_loss: 0.3481\n",
      "train\t epoch:1/3\t batch_index:163\t train_acc: 89.8\t train_loss: 0.3477\n",
      "train\t epoch:1/3\t batch_index:164\t train_acc: 89.8\t train_loss: 0.3476\n",
      "train\t epoch:1/3\t batch_index:165\t train_acc: 89.78\t train_loss: 0.3474\n",
      "train\t epoch:1/3\t batch_index:166\t train_acc: 89.8\t train_loss: 0.3472\n",
      "train\t epoch:1/3\t batch_index:167\t train_acc: 89.81\t train_loss: 0.3469\n",
      "train\t epoch:1/3\t batch_index:168\t train_acc: 89.81\t train_loss: 0.3468\n",
      "train\t epoch:1/3\t batch_index:169\t train_acc: 89.83\t train_loss: 0.3465\n",
      "train\t epoch:1/3\t batch_index:170\t train_acc: 89.85\t train_loss: 0.3463\n",
      "train\t epoch:1/3\t batch_index:171\t train_acc: 89.83\t train_loss: 0.3465\n",
      "train\t epoch:1/3\t batch_index:172\t train_acc: 89.83\t train_loss: 0.3464\n",
      "train\t epoch:1/3\t batch_index:173\t train_acc: 89.83\t train_loss: 0.3462\n",
      "train\t epoch:1/3\t batch_index:174\t train_acc: 89.87\t train_loss: 0.346\n",
      "train\t epoch:1/3\t batch_index:175\t train_acc: 89.88\t train_loss: 0.3458\n",
      "train\t epoch:1/3\t batch_index:176\t train_acc: 89.92\t train_loss: 0.3455\n",
      "train\t epoch:1/3\t batch_index:177\t train_acc: 89.92\t train_loss: 0.3452\n",
      "train\t epoch:1/3\t batch_index:178\t train_acc: 89.9\t train_loss: 0.3453\n",
      "train\t epoch:1/3\t batch_index:179\t train_acc: 89.88\t train_loss: 0.3453\n",
      "train\t epoch:1/3\t batch_index:180\t train_acc: 89.91\t train_loss: 0.3449\n",
      "train\t epoch:1/3\t batch_index:181\t train_acc: 89.93\t train_loss: 0.3445\n",
      "train\t epoch:1/3\t batch_index:182\t train_acc: 89.9\t train_loss: 0.3445\n",
      "train\t epoch:1/3\t batch_index:183\t train_acc: 89.93\t train_loss: 0.3444\n",
      "train\t epoch:1/3\t batch_index:184\t train_acc: 89.92\t train_loss: 0.3443\n",
      "train\t epoch:1/3\t batch_index:185\t train_acc: 89.92\t train_loss: 0.3442\n",
      "train\t epoch:1/3\t batch_index:186\t train_acc: 89.92\t train_loss: 0.3442\n",
      "train\t epoch:1/3\t batch_index:187\t train_acc: 89.94\t train_loss: 0.3439\n",
      "train\t epoch:1/3\t batch_index:188\t train_acc: 89.95\t train_loss: 0.3436\n",
      "train\t epoch:1/3\t batch_index:189\t train_acc: 89.93\t train_loss: 0.3436\n",
      "train\t epoch:1/3\t batch_index:190\t train_acc: 89.91\t train_loss: 0.3436\n",
      "train\t epoch:1/3\t batch_index:191\t train_acc: 89.91\t train_loss: 0.3435\n",
      "train\t epoch:1/3\t batch_index:192\t train_acc: 89.88\t train_loss: 0.3435\n",
      "train\t epoch:1/3\t batch_index:193\t train_acc: 89.86\t train_loss: 0.3437\n",
      "train\t epoch:1/3\t batch_index:194\t train_acc: 89.85\t train_loss: 0.3436\n",
      "train\t epoch:1/3\t batch_index:195\t train_acc: 89.84\t train_loss: 0.3436\n",
      "train\t epoch:1/3\t batch_index:196\t train_acc: 89.84\t train_loss: 0.3437\n",
      "train\t epoch:1/3\t batch_index:197\t train_acc: 89.84\t train_loss: 0.3437\n",
      "train\t epoch:1/3\t batch_index:198\t train_acc: 89.85\t train_loss: 0.3436\n",
      "train\t epoch:1/3\t batch_index:199\t train_acc: 89.88\t train_loss: 0.3434\n",
      "train\t epoch:1/3\t batch_index:200\t train_acc: 89.89\t train_loss: 0.343\n",
      "train\t epoch:1/3\t batch_index:201\t train_acc: 89.91\t train_loss: 0.3428\n",
      "train\t epoch:1/3\t batch_index:202\t train_acc: 89.9\t train_loss: 0.3427\n",
      "train\t epoch:1/3\t batch_index:203\t train_acc: 89.92\t train_loss: 0.3424\n",
      "train\t epoch:1/3\t batch_index:204\t train_acc: 89.93\t train_loss: 0.3423\n",
      "train\t epoch:1/3\t batch_index:205\t train_acc: 89.92\t train_loss: 0.3422\n",
      "train\t epoch:1/3\t batch_index:206\t train_acc: 89.9\t train_loss: 0.3422\n",
      "train\t epoch:1/3\t batch_index:207\t train_acc: 89.91\t train_loss: 0.3422\n",
      "train\t epoch:1/3\t batch_index:208\t train_acc: 89.9\t train_loss: 0.342\n",
      "train\t epoch:1/3\t batch_index:209\t train_acc: 89.9\t train_loss: 0.3419\n",
      "train\t epoch:1/3\t batch_index:210\t train_acc: 89.9\t train_loss: 0.3417\n",
      "train\t epoch:1/3\t batch_index:211\t train_acc: 89.92\t train_loss: 0.3414\n",
      "train\t epoch:1/3\t batch_index:212\t train_acc: 89.92\t train_loss: 0.3413\n",
      "train\t epoch:1/3\t batch_index:213\t train_acc: 89.93\t train_loss: 0.3411\n",
      "train\t epoch:1/3\t batch_index:214\t train_acc: 89.94\t train_loss: 0.3408\n",
      "train\t epoch:1/3\t batch_index:215\t train_acc: 89.94\t train_loss: 0.3408\n",
      "train\t epoch:1/3\t batch_index:216\t train_acc: 89.92\t train_loss: 0.3408\n",
      "train\t epoch:1/3\t batch_index:217\t train_acc: 89.93\t train_loss: 0.3406\n",
      "train\t epoch:1/3\t batch_index:218\t train_acc: 89.92\t train_loss: 0.3406\n",
      "train\t epoch:1/3\t batch_index:219\t train_acc: 89.93\t train_loss: 0.3404\n",
      "train\t epoch:1/3\t batch_index:220\t train_acc: 89.91\t train_loss: 0.3405\n",
      "train\t epoch:1/3\t batch_index:221\t train_acc: 89.92\t train_loss: 0.3403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\t epoch:1/3\t batch_index:222\t train_acc: 89.92\t train_loss: 0.3402\n",
      "train\t epoch:1/3\t batch_index:223\t train_acc: 89.9\t train_loss: 0.3402\n",
      "train\t epoch:1/3\t batch_index:224\t train_acc: 89.91\t train_loss: 0.3402\n",
      "train\t epoch:1/3\t batch_index:225\t train_acc: 89.9\t train_loss: 0.3403\n",
      "train\t epoch:1/3\t batch_index:226\t train_acc: 89.9\t train_loss: 0.3403\n",
      "train\t epoch:1/3\t batch_index:227\t train_acc: 89.93\t train_loss: 0.34\n",
      "train\t epoch:1/3\t batch_index:228\t train_acc: 89.91\t train_loss: 0.34\n",
      "train\t epoch:1/3\t batch_index:229\t train_acc: 89.91\t train_loss: 0.34\n",
      "train\t epoch:1/3\t batch_index:230\t train_acc: 89.9\t train_loss: 0.34\n",
      "train\t epoch:1/3\t batch_index:231\t train_acc: 89.89\t train_loss: 0.34\n",
      "train\t epoch:1/3\t batch_index:232\t train_acc: 89.9\t train_loss: 0.3398\n",
      "train\t epoch:1/3\t batch_index:233\t train_acc: 89.9\t train_loss: 0.3396\n",
      "train\t epoch:1/3\t batch_index:234\t train_acc: 89.9\t train_loss: 0.3397\n",
      "train\t epoch:1/3\t batch_index:235\t train_acc: 89.88\t train_loss: 0.3398\n",
      "train\t epoch:1/3\t batch_index:236\t train_acc: 89.86\t train_loss: 0.34\n",
      "train\t epoch:1/3\t batch_index:237\t train_acc: 89.87\t train_loss: 0.3398\n",
      "train\t epoch:1/3\t batch_index:238\t train_acc: 89.87\t train_loss: 0.3397\n",
      "train\t epoch:1/3\t batch_index:239\t train_acc: 89.85\t train_loss: 0.3399\n",
      "train\t epoch:1/3\t batch_index:240\t train_acc: 89.86\t train_loss: 0.3397\n",
      "train\t epoch:1/3\t batch_index:241\t train_acc: 89.87\t train_loss: 0.3396\n",
      "train\t epoch:1/3\t batch_index:242\t train_acc: 89.89\t train_loss: 0.3395\n",
      "train\t epoch:1/3\t batch_index:243\t train_acc: 89.92\t train_loss: 0.3392\n",
      "train\t epoch:1/3\t batch_index:244\t train_acc: 89.91\t train_loss: 0.3391\n",
      "train\t epoch:1/3\t batch_index:245\t train_acc: 89.92\t train_loss: 0.3389\n",
      "train\t epoch:1/3\t batch_index:246\t train_acc: 89.93\t train_loss: 0.3387\n",
      "train\t epoch:1/3\t batch_index:247\t train_acc: 89.95\t train_loss: 0.3384\n",
      "train\t epoch:1/3\t batch_index:248\t train_acc: 89.95\t train_loss: 0.3382\n",
      "train\t epoch:1/3\t batch_index:249\t train_acc: 89.97\t train_loss: 0.3379\n",
      "train\t epoch:1/3\t batch_index:250\t train_acc: 89.97\t train_loss: 0.3378\n",
      "train\t epoch:1/3\t batch_index:251\t train_acc: 89.98\t train_loss: 0.3378\n",
      "train\t epoch:1/3\t batch_index:252\t train_acc: 89.97\t train_loss: 0.3378\n",
      "train\t epoch:1/3\t batch_index:253\t train_acc: 89.98\t train_loss: 0.3378\n",
      "train\t epoch:1/3\t batch_index:254\t train_acc: 89.98\t train_loss: 0.3376\n",
      "train\t epoch:1/3\t batch_index:255\t train_acc: 89.98\t train_loss: 0.3374\n",
      "train\t epoch:1/3\t batch_index:256\t train_acc: 90.0\t train_loss: 0.3371\n",
      "train\t epoch:1/3\t batch_index:257\t train_acc: 90.02\t train_loss: 0.3369\n",
      "train\t epoch:1/3\t batch_index:258\t train_acc: 90.03\t train_loss: 0.3367\n",
      "train\t epoch:1/3\t batch_index:259\t train_acc: 90.02\t train_loss: 0.3367\n",
      "train\t epoch:1/3\t batch_index:260\t train_acc: 90.04\t train_loss: 0.3364\n",
      "train\t epoch:1/3\t batch_index:261\t train_acc: 90.03\t train_loss: 0.3363\n",
      "train\t epoch:1/3\t batch_index:262\t train_acc: 90.01\t train_loss: 0.3364\n",
      "train\t epoch:1/3\t batch_index:263\t train_acc: 90.0\t train_loss: 0.3363\n",
      "train\t epoch:1/3\t batch_index:264\t train_acc: 89.98\t train_loss: 0.3363\n",
      "train\t epoch:1/3\t batch_index:265\t train_acc: 89.99\t train_loss: 0.3361\n",
      "train\t epoch:1/3\t batch_index:266\t train_acc: 90.0\t train_loss: 0.3359\n",
      "train\t epoch:1/3\t batch_index:267\t train_acc: 89.99\t train_loss: 0.3359\n",
      "train\t epoch:1/3\t batch_index:268\t train_acc: 89.96\t train_loss: 0.3362\n",
      "train\t epoch:1/3\t batch_index:269\t train_acc: 89.96\t train_loss: 0.3361\n",
      "train\t epoch:1/3\t batch_index:270\t train_acc: 89.96\t train_loss: 0.336\n",
      "train\t epoch:1/3\t batch_index:271\t train_acc: 89.96\t train_loss: 0.336\n",
      "train\t epoch:1/3\t batch_index:272\t train_acc: 89.96\t train_loss: 0.3358\n",
      "train\t epoch:1/3\t batch_index:273\t train_acc: 89.97\t train_loss: 0.3355\n",
      "train\t epoch:1/3\t batch_index:274\t train_acc: 89.97\t train_loss: 0.3354\n",
      "train\t epoch:1/3\t batch_index:275\t train_acc: 89.98\t train_loss: 0.3353\n",
      "train\t epoch:1/3\t batch_index:276\t train_acc: 89.99\t train_loss: 0.3351\n",
      "train\t epoch:1/3\t batch_index:277\t train_acc: 89.99\t train_loss: 0.3351\n",
      "train\t epoch:1/3\t batch_index:278\t train_acc: 89.98\t train_loss: 0.3351\n",
      "train\t epoch:1/3\t batch_index:279\t train_acc: 89.98\t train_loss: 0.335\n",
      "train\t epoch:1/3\t batch_index:280\t train_acc: 89.98\t train_loss: 0.3349\n",
      "train\t epoch:1/3\t batch_index:281\t train_acc: 89.97\t train_loss: 0.3349\n",
      "train\t epoch:1/3\t batch_index:282\t train_acc: 89.98\t train_loss: 0.3347\n",
      "train\t epoch:1/3\t batch_index:283\t train_acc: 89.98\t train_loss: 0.3346\n",
      "train\t epoch:1/3\t batch_index:284\t train_acc: 89.97\t train_loss: 0.3345\n",
      "train\t epoch:1/3\t batch_index:285\t train_acc: 89.98\t train_loss: 0.3344\n",
      "train\t epoch:1/3\t batch_index:286\t train_acc: 89.99\t train_loss: 0.3342\n",
      "train\t epoch:1/3\t batch_index:287\t train_acc: 89.98\t train_loss: 0.3341\n",
      "train\t epoch:1/3\t batch_index:288\t train_acc: 90.0\t train_loss: 0.334\n",
      "train\t epoch:1/3\t batch_index:289\t train_acc: 90.01\t train_loss: 0.3337\n",
      "train\t epoch:1/3\t batch_index:290\t train_acc: 90.01\t train_loss: 0.3336\n",
      "train\t epoch:1/3\t batch_index:291\t train_acc: 90.01\t train_loss: 0.3335\n",
      "train\t epoch:1/3\t batch_index:292\t train_acc: 90.01\t train_loss: 0.3333\n",
      "train\t epoch:1/3\t batch_index:293\t train_acc: 90.02\t train_loss: 0.3333\n",
      "train\t epoch:1/3\t batch_index:294\t train_acc: 90.01\t train_loss: 0.3334\n",
      "train\t epoch:1/3\t batch_index:295\t train_acc: 90.02\t train_loss: 0.3331\n",
      "train\t epoch:1/3\t batch_index:296\t train_acc: 90.02\t train_loss: 0.3329\n",
      "train\t epoch:1/3\t batch_index:297\t train_acc: 90.03\t train_loss: 0.3328\n",
      "train\t epoch:1/3\t batch_index:298\t train_acc: 90.05\t train_loss: 0.3326\n",
      "train\t epoch:1/3\t batch_index:299\t train_acc: 90.06\t train_loss: 0.3324\n",
      "train\t epoch:1/3\t batch_index:300\t train_acc: 90.06\t train_loss: 0.3323\n",
      "train\t epoch:1/3\t batch_index:301\t train_acc: 90.07\t train_loss: 0.3321\n",
      "train\t epoch:1/3\t batch_index:302\t train_acc: 90.07\t train_loss: 0.332\n",
      "train\t epoch:1/3\t batch_index:303\t train_acc: 90.07\t train_loss: 0.3319\n",
      "train\t epoch:1/3\t batch_index:304\t train_acc: 90.07\t train_loss: 0.3318\n",
      "train\t epoch:1/3\t batch_index:305\t train_acc: 90.08\t train_loss: 0.3315\n",
      "val\t epoch:1/3\t batch_index:0\t val_acc: 90.62\t val_loss: 0.3162\n",
      "val\t epoch:1/3\t batch_index:1\t val_acc: 89.84\t val_loss: 0.3165\n",
      "val\t epoch:1/3\t batch_index:2\t val_acc: 88.8\t val_loss: 0.3285\n",
      "val\t epoch:1/3\t batch_index:3\t val_acc: 88.48\t val_loss: 0.3308\n",
      "val\t epoch:1/3\t batch_index:4\t val_acc: 89.38\t val_loss: 0.3238\n",
      "val\t epoch:1/3\t batch_index:5\t val_acc: 89.84\t val_loss: 0.3196\n",
      "val\t epoch:1/3\t batch_index:6\t val_acc: 89.84\t val_loss: 0.3225\n",
      "val\t epoch:1/3\t batch_index:7\t val_acc: 89.94\t val_loss: 0.3222\n",
      "val\t epoch:1/3\t batch_index:8\t val_acc: 90.02\t val_loss: 0.3181\n",
      "val\t epoch:1/3\t batch_index:9\t val_acc: 90.47\t val_loss: 0.3148\n",
      "val\t epoch:1/3\t batch_index:10\t val_acc: 90.7\t val_loss: 0.3152\n",
      "val\t epoch:1/3\t batch_index:11\t val_acc: 90.82\t val_loss: 0.3164\n",
      "val\t epoch:1/3\t batch_index:12\t val_acc: 90.62\t val_loss: 0.3189\n",
      "val\t epoch:1/3\t batch_index:13\t val_acc: 90.85\t val_loss: 0.3147\n",
      "val\t epoch:1/3\t batch_index:14\t val_acc: 90.73\t val_loss: 0.3146\n",
      "val\t epoch:1/3\t batch_index:15\t val_acc: 90.87\t val_loss: 0.312\n",
      "val\t epoch:1/3\t batch_index:16\t val_acc: 90.99\t val_loss: 0.3103\n",
      "val\t epoch:1/3\t batch_index:17\t val_acc: 90.97\t val_loss: 0.3099\n",
      "val\t epoch:1/3\t batch_index:18\t val_acc: 90.91\t val_loss: 0.3091\n",
      "val\t epoch:1/3\t batch_index:19\t val_acc: 91.02\t val_loss: 0.3075\n",
      "val\t epoch:1/3\t batch_index:20\t val_acc: 90.96\t val_loss: 0.3064\n",
      "val\t epoch:1/3\t batch_index:21\t val_acc: 90.91\t val_loss: 0.3066\n",
      "val\t epoch:1/3\t batch_index:22\t val_acc: 90.83\t val_loss: 0.3062\n",
      "val\t epoch:1/3\t batch_index:23\t val_acc: 90.76\t val_loss: 0.3074\n",
      "val\t epoch:1/3\t batch_index:24\t val_acc: 90.81\t val_loss: 0.307\n",
      "val\t epoch:1/3\t batch_index:25\t val_acc: 90.53\t val_loss: 0.3079\n",
      "val\t epoch:1/3\t batch_index:26\t val_acc: 90.42\t val_loss: 0.3093\n",
      "val\t epoch:1/3\t batch_index:27\t val_acc: 90.32\t val_loss: 0.3094\n",
      "val\t epoch:1/3\t batch_index:28\t val_acc: 90.44\t val_loss: 0.3089\n",
      "val\t epoch:1/3\t batch_index:29\t val_acc: 90.34\t val_loss: 0.31\n",
      "val\t epoch:1/3\t batch_index:30\t val_acc: 90.3\t val_loss: 0.3105\n",
      "val\t epoch:1/3\t batch_index:31\t val_acc: 90.23\t val_loss: 0.3109\n",
      "val\t epoch:1/3\t batch_index:32\t val_acc: 90.27\t val_loss: 0.3101\n",
      "val\t epoch:1/3\t batch_index:33\t val_acc: 90.21\t val_loss: 0.311\n",
      "val\t epoch:1/3\t batch_index:34\t val_acc: 90.09\t val_loss: 0.3124\n",
      "val\t epoch:1/3\t batch_index:35\t val_acc: 90.04\t val_loss: 0.3127\n",
      "val\t epoch:1/3\t batch_index:36\t val_acc: 90.08\t val_loss: 0.3129\n",
      "val\t epoch:1/3\t batch_index:37\t val_acc: 90.17\t val_loss: 0.3122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val\t epoch:1/3\t batch_index:38\t val_acc: 90.1\t val_loss: 0.3128\n",
      "val\t epoch:1/3\t batch_index:39\t val_acc: 90.14\t val_loss: 0.3131\n",
      "val\t epoch:1/3\t batch_index:40\t val_acc: 90.19\t val_loss: 0.3126\n",
      "val\t epoch:1/3\t batch_index:41\t val_acc: 90.18\t val_loss: 0.3124\n",
      "val\t epoch:1/3\t batch_index:42\t val_acc: 90.19\t val_loss: 0.312\n",
      "val\t epoch:1/3\t batch_index:43\t val_acc: 90.22\t val_loss: 0.3113\n",
      "val\t epoch:1/3\t batch_index:44\t val_acc: 90.26\t val_loss: 0.3118\n",
      "val\t epoch:1/3\t batch_index:45\t val_acc: 90.32\t val_loss: 0.3103\n",
      "val\t epoch:1/3\t batch_index:46\t val_acc: 90.31\t val_loss: 0.3095\n",
      "val\t epoch:1/3\t batch_index:47\t val_acc: 90.32\t val_loss: 0.3099\n",
      "val\t epoch:1/3\t batch_index:48\t val_acc: 90.29\t val_loss: 0.31\n",
      "val\t epoch:1/3\t batch_index:49\t val_acc: 90.25\t val_loss: 0.3098\n",
      "val\t epoch:1/3\t batch_index:50\t val_acc: 90.21\t val_loss: 0.31\n",
      "val\t epoch:1/3\t batch_index:51\t val_acc: 90.19\t val_loss: 0.31\n",
      "val\t epoch:1/3\t batch_index:52\t val_acc: 90.14\t val_loss: 0.3106\n",
      "val\t epoch:1/3\t batch_index:53\t val_acc: 90.19\t val_loss: 0.3099\n",
      "val\t epoch:1/3\t batch_index:54\t val_acc: 90.14\t val_loss: 0.3107\n",
      "val\t epoch:1/3\t batch_index:55\t val_acc: 90.18\t val_loss: 0.3103\n",
      "val\t epoch:1/3\t batch_index:56\t val_acc: 90.16\t val_loss: 0.3109\n",
      "val\t epoch:1/3\t batch_index:57\t val_acc: 90.15\t val_loss: 0.3113\n",
      "val\t epoch:1/3\t batch_index:58\t val_acc: 90.21\t val_loss: 0.311\n",
      "val\t epoch:1/3\t batch_index:59\t val_acc: 90.23\t val_loss: 0.3114\n",
      "val\t epoch:1/3\t batch_index:60\t val_acc: 90.25\t val_loss: 0.3115\n",
      "val\t epoch:1/3\t batch_index:61\t val_acc: 90.21\t val_loss: 0.3118\n",
      "val\t epoch:1/3\t batch_index:62\t val_acc: 90.2\t val_loss: 0.3124\n",
      "val\t epoch:1/3\t batch_index:63\t val_acc: 90.32\t val_loss: 0.3114\n",
      "val\t epoch:1/3\t batch_index:64\t val_acc: 90.37\t val_loss: 0.3109\n",
      "train\t epoch:2/3\t batch_index:0\t train_acc: 92.97\t train_loss: 0.283\n",
      "train\t epoch:2/3\t batch_index:1\t train_acc: 92.58\t train_loss: 0.2813\n",
      "train\t epoch:2/3\t batch_index:2\t train_acc: 92.19\t train_loss: 0.2787\n",
      "train\t epoch:2/3\t batch_index:3\t train_acc: 92.77\t train_loss: 0.2729\n",
      "train\t epoch:2/3\t batch_index:4\t train_acc: 92.97\t train_loss: 0.2775\n",
      "train\t epoch:2/3\t batch_index:5\t train_acc: 92.45\t train_loss: 0.2774\n",
      "train\t epoch:2/3\t batch_index:6\t train_acc: 92.75\t train_loss: 0.274\n",
      "train\t epoch:2/3\t batch_index:7\t train_acc: 92.97\t train_loss: 0.2729\n",
      "train\t epoch:2/3\t batch_index:8\t train_acc: 91.93\t train_loss: 0.2794\n",
      "train\t epoch:2/3\t batch_index:9\t train_acc: 91.95\t train_loss: 0.2777\n",
      "train\t epoch:2/3\t batch_index:10\t train_acc: 92.19\t train_loss: 0.2787\n",
      "train\t epoch:2/3\t batch_index:11\t train_acc: 92.38\t train_loss: 0.2794\n",
      "train\t epoch:2/3\t batch_index:12\t train_acc: 92.49\t train_loss: 0.2799\n",
      "train\t epoch:2/3\t batch_index:13\t train_acc: 92.69\t train_loss: 0.2778\n",
      "train\t epoch:2/3\t batch_index:14\t train_acc: 92.86\t train_loss: 0.2758\n",
      "train\t epoch:2/3\t batch_index:15\t train_acc: 93.02\t train_loss: 0.2761\n",
      "train\t epoch:2/3\t batch_index:16\t train_acc: 92.69\t train_loss: 0.2771\n",
      "train\t epoch:2/3\t batch_index:17\t train_acc: 92.45\t train_loss: 0.2785\n",
      "train\t epoch:2/3\t batch_index:18\t train_acc: 92.31\t train_loss: 0.2793\n",
      "train\t epoch:2/3\t batch_index:19\t train_acc: 92.23\t train_loss: 0.2812\n",
      "train\t epoch:2/3\t batch_index:20\t train_acc: 92.26\t train_loss: 0.2813\n",
      "train\t epoch:2/3\t batch_index:21\t train_acc: 92.08\t train_loss: 0.2821\n",
      "train\t epoch:2/3\t batch_index:22\t train_acc: 92.19\t train_loss: 0.2809\n",
      "train\t epoch:2/3\t batch_index:23\t train_acc: 92.25\t train_loss: 0.2815\n",
      "train\t epoch:2/3\t batch_index:24\t train_acc: 92.38\t train_loss: 0.2808\n",
      "train\t epoch:2/3\t batch_index:25\t train_acc: 92.49\t train_loss: 0.28\n",
      "train\t epoch:2/3\t batch_index:26\t train_acc: 92.39\t train_loss: 0.2799\n",
      "train\t epoch:2/3\t batch_index:27\t train_acc: 92.33\t train_loss: 0.281\n",
      "train\t epoch:2/3\t batch_index:28\t train_acc: 92.32\t train_loss: 0.2809\n",
      "train\t epoch:2/3\t batch_index:29\t train_acc: 92.14\t train_loss: 0.2822\n",
      "train\t epoch:2/3\t batch_index:30\t train_acc: 92.16\t train_loss: 0.2826\n",
      "train\t epoch:2/3\t batch_index:31\t train_acc: 92.09\t train_loss: 0.2836\n",
      "train\t epoch:2/3\t batch_index:32\t train_acc: 92.19\t train_loss: 0.2835\n",
      "train\t epoch:2/3\t batch_index:33\t train_acc: 92.19\t train_loss: 0.2831\n",
      "train\t epoch:2/3\t batch_index:34\t train_acc: 92.12\t train_loss: 0.2847\n",
      "train\t epoch:2/3\t batch_index:35\t train_acc: 92.12\t train_loss: 0.2845\n",
      "train\t epoch:2/3\t batch_index:36\t train_acc: 92.17\t train_loss: 0.2834\n",
      "train\t epoch:2/3\t batch_index:37\t train_acc: 92.15\t train_loss: 0.2837\n",
      "train\t epoch:2/3\t batch_index:38\t train_acc: 92.13\t train_loss: 0.2838\n",
      "train\t epoch:2/3\t batch_index:39\t train_acc: 92.11\t train_loss: 0.2839\n",
      "train\t epoch:2/3\t batch_index:40\t train_acc: 91.94\t train_loss: 0.2854\n",
      "train\t epoch:2/3\t batch_index:41\t train_acc: 92.02\t train_loss: 0.2858\n",
      "train\t epoch:2/3\t batch_index:42\t train_acc: 91.97\t train_loss: 0.2857\n",
      "train\t epoch:2/3\t batch_index:43\t train_acc: 92.05\t train_loss: 0.2847\n",
      "train\t epoch:2/3\t batch_index:44\t train_acc: 92.01\t train_loss: 0.2846\n",
      "train\t epoch:2/3\t batch_index:45\t train_acc: 91.95\t train_loss: 0.286\n",
      "train\t epoch:2/3\t batch_index:46\t train_acc: 91.92\t train_loss: 0.2859\n",
      "train\t epoch:2/3\t batch_index:47\t train_acc: 91.88\t train_loss: 0.2862\n",
      "train\t epoch:2/3\t batch_index:48\t train_acc: 91.84\t train_loss: 0.2859\n",
      "train\t epoch:2/3\t batch_index:49\t train_acc: 91.86\t train_loss: 0.2856\n",
      "train\t epoch:2/3\t batch_index:50\t train_acc: 91.73\t train_loss: 0.2868\n",
      "train\t epoch:2/3\t batch_index:51\t train_acc: 91.66\t train_loss: 0.2864\n",
      "train\t epoch:2/3\t batch_index:52\t train_acc: 91.63\t train_loss: 0.2867\n",
      "train\t epoch:2/3\t batch_index:53\t train_acc: 91.7\t train_loss: 0.2865\n",
      "train\t epoch:2/3\t batch_index:54\t train_acc: 91.72\t train_loss: 0.2866\n",
      "train\t epoch:2/3\t batch_index:55\t train_acc: 91.7\t train_loss: 0.2867\n",
      "train\t epoch:2/3\t batch_index:56\t train_acc: 91.75\t train_loss: 0.2872\n",
      "train\t epoch:2/3\t batch_index:57\t train_acc: 91.76\t train_loss: 0.2876\n",
      "train\t epoch:2/3\t batch_index:58\t train_acc: 91.78\t train_loss: 0.2871\n",
      "train\t epoch:2/3\t batch_index:59\t train_acc: 91.78\t train_loss: 0.2874\n",
      "train\t epoch:2/3\t batch_index:60\t train_acc: 91.79\t train_loss: 0.2872\n",
      "train\t epoch:2/3\t batch_index:61\t train_acc: 91.78\t train_loss: 0.2872\n",
      "train\t epoch:2/3\t batch_index:62\t train_acc: 91.79\t train_loss: 0.2871\n",
      "train\t epoch:2/3\t batch_index:63\t train_acc: 91.78\t train_loss: 0.287\n",
      "train\t epoch:2/3\t batch_index:64\t train_acc: 91.78\t train_loss: 0.2872\n",
      "train\t epoch:2/3\t batch_index:65\t train_acc: 91.75\t train_loss: 0.2872\n",
      "train\t epoch:2/3\t batch_index:66\t train_acc: 91.7\t train_loss: 0.2878\n",
      "train\t epoch:2/3\t batch_index:67\t train_acc: 91.69\t train_loss: 0.2877\n",
      "train\t epoch:2/3\t batch_index:68\t train_acc: 91.7\t train_loss: 0.2876\n",
      "train\t epoch:2/3\t batch_index:69\t train_acc: 91.65\t train_loss: 0.2877\n",
      "train\t epoch:2/3\t batch_index:70\t train_acc: 91.6\t train_loss: 0.2875\n",
      "train\t epoch:2/3\t batch_index:71\t train_acc: 91.6\t train_loss: 0.2874\n",
      "train\t epoch:2/3\t batch_index:72\t train_acc: 91.62\t train_loss: 0.2874\n",
      "train\t epoch:2/3\t batch_index:73\t train_acc: 91.64\t train_loss: 0.2875\n",
      "train\t epoch:2/3\t batch_index:74\t train_acc: 91.64\t train_loss: 0.2877\n",
      "train\t epoch:2/3\t batch_index:75\t train_acc: 91.68\t train_loss: 0.2868\n",
      "train\t epoch:2/3\t batch_index:76\t train_acc: 91.68\t train_loss: 0.2864\n",
      "train\t epoch:2/3\t batch_index:77\t train_acc: 91.66\t train_loss: 0.2865\n",
      "train\t epoch:2/3\t batch_index:78\t train_acc: 91.6\t train_loss: 0.2868\n",
      "train\t epoch:2/3\t batch_index:79\t train_acc: 91.57\t train_loss: 0.2869\n",
      "train\t epoch:2/3\t batch_index:80\t train_acc: 91.56\t train_loss: 0.2868\n",
      "train\t epoch:2/3\t batch_index:81\t train_acc: 91.53\t train_loss: 0.287\n",
      "train\t epoch:2/3\t batch_index:82\t train_acc: 91.56\t train_loss: 0.2869\n",
      "train\t epoch:2/3\t batch_index:83\t train_acc: 91.58\t train_loss: 0.2864\n",
      "train\t epoch:2/3\t batch_index:84\t train_acc: 91.57\t train_loss: 0.2866\n",
      "train\t epoch:2/3\t batch_index:85\t train_acc: 91.59\t train_loss: 0.2867\n",
      "train\t epoch:2/3\t batch_index:86\t train_acc: 91.59\t train_loss: 0.2866\n",
      "train\t epoch:2/3\t batch_index:87\t train_acc: 91.57\t train_loss: 0.2866\n",
      "train\t epoch:2/3\t batch_index:88\t train_acc: 91.62\t train_loss: 0.286\n",
      "train\t epoch:2/3\t batch_index:89\t train_acc: 91.61\t train_loss: 0.2862\n",
      "train\t epoch:2/3\t batch_index:90\t train_acc: 91.59\t train_loss: 0.2865\n",
      "train\t epoch:2/3\t batch_index:91\t train_acc: 91.61\t train_loss: 0.2861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\t epoch:2/3\t batch_index:92\t train_acc: 91.63\t train_loss: 0.2857\n",
      "train\t epoch:2/3\t batch_index:93\t train_acc: 91.62\t train_loss: 0.2859\n",
      "train\t epoch:2/3\t batch_index:94\t train_acc: 91.64\t train_loss: 0.2855\n",
      "train\t epoch:2/3\t batch_index:95\t train_acc: 91.65\t train_loss: 0.2853\n",
      "train\t epoch:2/3\t batch_index:96\t train_acc: 91.69\t train_loss: 0.2848\n",
      "train\t epoch:2/3\t batch_index:97\t train_acc: 91.68\t train_loss: 0.2849\n",
      "train\t epoch:2/3\t batch_index:98\t train_acc: 91.67\t train_loss: 0.285\n",
      "train\t epoch:2/3\t batch_index:99\t train_acc: 91.65\t train_loss: 0.2848\n",
      "train\t epoch:2/3\t batch_index:100\t train_acc: 91.68\t train_loss: 0.2846\n",
      "train\t epoch:2/3\t batch_index:101\t train_acc: 91.65\t train_loss: 0.2844\n",
      "train\t epoch:2/3\t batch_index:102\t train_acc: 91.66\t train_loss: 0.2844\n",
      "train\t epoch:2/3\t batch_index:103\t train_acc: 91.66\t train_loss: 0.2844\n",
      "train\t epoch:2/3\t batch_index:104\t train_acc: 91.64\t train_loss: 0.2845\n",
      "train\t epoch:2/3\t batch_index:105\t train_acc: 91.64\t train_loss: 0.2842\n",
      "train\t epoch:2/3\t batch_index:106\t train_acc: 91.63\t train_loss: 0.2844\n",
      "train\t epoch:2/3\t batch_index:107\t train_acc: 91.63\t train_loss: 0.2844\n",
      "train\t epoch:2/3\t batch_index:108\t train_acc: 91.61\t train_loss: 0.2843\n",
      "train\t epoch:2/3\t batch_index:109\t train_acc: 91.6\t train_loss: 0.2846\n",
      "train\t epoch:2/3\t batch_index:110\t train_acc: 91.61\t train_loss: 0.2842\n",
      "train\t epoch:2/3\t batch_index:111\t train_acc: 91.62\t train_loss: 0.2839\n",
      "train\t epoch:2/3\t batch_index:112\t train_acc: 91.63\t train_loss: 0.2839\n",
      "train\t epoch:2/3\t batch_index:113\t train_acc: 91.63\t train_loss: 0.284\n",
      "train\t epoch:2/3\t batch_index:114\t train_acc: 91.62\t train_loss: 0.2841\n",
      "train\t epoch:2/3\t batch_index:115\t train_acc: 91.62\t train_loss: 0.2842\n",
      "train\t epoch:2/3\t batch_index:116\t train_acc: 91.64\t train_loss: 0.2841\n",
      "train\t epoch:2/3\t batch_index:117\t train_acc: 91.64\t train_loss: 0.2841\n",
      "train\t epoch:2/3\t batch_index:118\t train_acc: 91.64\t train_loss: 0.2841\n",
      "train\t epoch:2/3\t batch_index:119\t train_acc: 91.64\t train_loss: 0.284\n",
      "train\t epoch:2/3\t batch_index:120\t train_acc: 91.63\t train_loss: 0.284\n",
      "train\t epoch:2/3\t batch_index:121\t train_acc: 91.63\t train_loss: 0.2837\n",
      "train\t epoch:2/3\t batch_index:122\t train_acc: 91.62\t train_loss: 0.2838\n",
      "train\t epoch:2/3\t batch_index:123\t train_acc: 91.65\t train_loss: 0.2835\n",
      "train\t epoch:2/3\t batch_index:124\t train_acc: 91.61\t train_loss: 0.2837\n",
      "train\t epoch:2/3\t batch_index:125\t train_acc: 91.6\t train_loss: 0.2837\n",
      "train\t epoch:2/3\t batch_index:126\t train_acc: 91.6\t train_loss: 0.2837\n",
      "train\t epoch:2/3\t batch_index:127\t train_acc: 91.61\t train_loss: 0.2835\n",
      "train\t epoch:2/3\t batch_index:128\t train_acc: 91.59\t train_loss: 0.2836\n",
      "train\t epoch:2/3\t batch_index:129\t train_acc: 91.55\t train_loss: 0.2839\n",
      "train\t epoch:2/3\t batch_index:130\t train_acc: 91.55\t train_loss: 0.2839\n",
      "train\t epoch:2/3\t batch_index:131\t train_acc: 91.56\t train_loss: 0.2841\n",
      "train\t epoch:2/3\t batch_index:132\t train_acc: 91.54\t train_loss: 0.2842\n",
      "train\t epoch:2/3\t batch_index:133\t train_acc: 91.58\t train_loss: 0.2839\n",
      "train\t epoch:2/3\t batch_index:134\t train_acc: 91.59\t train_loss: 0.284\n",
      "train\t epoch:2/3\t batch_index:135\t train_acc: 91.57\t train_loss: 0.284\n",
      "train\t epoch:2/3\t batch_index:136\t train_acc: 91.59\t train_loss: 0.2835\n",
      "train\t epoch:2/3\t batch_index:137\t train_acc: 91.58\t train_loss: 0.2837\n",
      "train\t epoch:2/3\t batch_index:138\t train_acc: 91.57\t train_loss: 0.2836\n",
      "train\t epoch:2/3\t batch_index:139\t train_acc: 91.58\t train_loss: 0.2833\n",
      "train\t epoch:2/3\t batch_index:140\t train_acc: 91.58\t train_loss: 0.2834\n",
      "train\t epoch:2/3\t batch_index:141\t train_acc: 91.59\t train_loss: 0.2833\n",
      "train\t epoch:2/3\t batch_index:142\t train_acc: 91.58\t train_loss: 0.2834\n",
      "train\t epoch:2/3\t batch_index:143\t train_acc: 91.59\t train_loss: 0.2832\n",
      "train\t epoch:2/3\t batch_index:144\t train_acc: 91.56\t train_loss: 0.2832\n",
      "train\t epoch:2/3\t batch_index:145\t train_acc: 91.52\t train_loss: 0.2836\n",
      "train\t epoch:2/3\t batch_index:146\t train_acc: 91.51\t train_loss: 0.2837\n",
      "train\t epoch:2/3\t batch_index:147\t train_acc: 91.49\t train_loss: 0.2838\n",
      "train\t epoch:2/3\t batch_index:148\t train_acc: 91.49\t train_loss: 0.2836\n",
      "train\t epoch:2/3\t batch_index:149\t train_acc: 91.51\t train_loss: 0.2835\n",
      "train\t epoch:2/3\t batch_index:150\t train_acc: 91.51\t train_loss: 0.2834\n",
      "train\t epoch:2/3\t batch_index:151\t train_acc: 91.53\t train_loss: 0.2831\n",
      "train\t epoch:2/3\t batch_index:152\t train_acc: 91.53\t train_loss: 0.2831\n",
      "train\t epoch:2/3\t batch_index:153\t train_acc: 91.53\t train_loss: 0.2829\n",
      "train\t epoch:2/3\t batch_index:154\t train_acc: 91.55\t train_loss: 0.2826\n",
      "train\t epoch:2/3\t batch_index:155\t train_acc: 91.55\t train_loss: 0.2824\n",
      "train\t epoch:2/3\t batch_index:156\t train_acc: 91.53\t train_loss: 0.2825\n",
      "train\t epoch:2/3\t batch_index:157\t train_acc: 91.53\t train_loss: 0.2822\n",
      "train\t epoch:2/3\t batch_index:158\t train_acc: 91.54\t train_loss: 0.282\n",
      "train\t epoch:2/3\t batch_index:159\t train_acc: 91.56\t train_loss: 0.2817\n",
      "train\t epoch:2/3\t batch_index:160\t train_acc: 91.55\t train_loss: 0.2818\n",
      "train\t epoch:2/3\t batch_index:161\t train_acc: 91.54\t train_loss: 0.2818\n",
      "train\t epoch:2/3\t batch_index:162\t train_acc: 91.51\t train_loss: 0.282\n",
      "train\t epoch:2/3\t batch_index:163\t train_acc: 91.5\t train_loss: 0.282\n",
      "train\t epoch:2/3\t batch_index:164\t train_acc: 91.53\t train_loss: 0.2815\n",
      "train\t epoch:2/3\t batch_index:165\t train_acc: 91.54\t train_loss: 0.2813\n",
      "train\t epoch:2/3\t batch_index:166\t train_acc: 91.56\t train_loss: 0.281\n",
      "train\t epoch:2/3\t batch_index:167\t train_acc: 91.54\t train_loss: 0.2811\n",
      "train\t epoch:2/3\t batch_index:168\t train_acc: 91.55\t train_loss: 0.281\n",
      "train\t epoch:2/3\t batch_index:169\t train_acc: 91.57\t train_loss: 0.2809\n",
      "train\t epoch:2/3\t batch_index:170\t train_acc: 91.57\t train_loss: 0.281\n",
      "train\t epoch:2/3\t batch_index:171\t train_acc: 91.57\t train_loss: 0.2808\n",
      "train\t epoch:2/3\t batch_index:172\t train_acc: 91.56\t train_loss: 0.2809\n",
      "train\t epoch:2/3\t batch_index:173\t train_acc: 91.58\t train_loss: 0.2808\n",
      "train\t epoch:2/3\t batch_index:174\t train_acc: 91.56\t train_loss: 0.2809\n",
      "train\t epoch:2/3\t batch_index:175\t train_acc: 91.55\t train_loss: 0.2811\n",
      "train\t epoch:2/3\t batch_index:176\t train_acc: 91.56\t train_loss: 0.2811\n",
      "train\t epoch:2/3\t batch_index:177\t train_acc: 91.55\t train_loss: 0.2811\n",
      "train\t epoch:2/3\t batch_index:178\t train_acc: 91.55\t train_loss: 0.2812\n",
      "train\t epoch:2/3\t batch_index:179\t train_acc: 91.55\t train_loss: 0.2813\n",
      "train\t epoch:2/3\t batch_index:180\t train_acc: 91.56\t train_loss: 0.2811\n",
      "train\t epoch:2/3\t batch_index:181\t train_acc: 91.59\t train_loss: 0.2807\n",
      "train\t epoch:2/3\t batch_index:182\t train_acc: 91.57\t train_loss: 0.2808\n",
      "train\t epoch:2/3\t batch_index:183\t train_acc: 91.55\t train_loss: 0.2808\n",
      "train\t epoch:2/3\t batch_index:184\t train_acc: 91.55\t train_loss: 0.2806\n",
      "train\t epoch:2/3\t batch_index:185\t train_acc: 91.57\t train_loss: 0.2804\n",
      "train\t epoch:2/3\t batch_index:186\t train_acc: 91.58\t train_loss: 0.2803\n",
      "train\t epoch:2/3\t batch_index:187\t train_acc: 91.59\t train_loss: 0.2801\n",
      "train\t epoch:2/3\t batch_index:188\t train_acc: 91.59\t train_loss: 0.28\n",
      "train\t epoch:2/3\t batch_index:189\t train_acc: 91.59\t train_loss: 0.2801\n",
      "train\t epoch:2/3\t batch_index:190\t train_acc: 91.6\t train_loss: 0.2799\n",
      "train\t epoch:2/3\t batch_index:191\t train_acc: 91.58\t train_loss: 0.28\n",
      "train\t epoch:2/3\t batch_index:192\t train_acc: 91.58\t train_loss: 0.2801\n",
      "train\t epoch:2/3\t batch_index:193\t train_acc: 91.58\t train_loss: 0.2801\n",
      "train\t epoch:2/3\t batch_index:194\t train_acc: 91.59\t train_loss: 0.2802\n",
      "train\t epoch:2/3\t batch_index:195\t train_acc: 91.58\t train_loss: 0.2801\n",
      "train\t epoch:2/3\t batch_index:196\t train_acc: 91.59\t train_loss: 0.2798\n",
      "train\t epoch:2/3\t batch_index:197\t train_acc: 91.58\t train_loss: 0.2798\n",
      "train\t epoch:2/3\t batch_index:198\t train_acc: 91.58\t train_loss: 0.2799\n",
      "train\t epoch:2/3\t batch_index:199\t train_acc: 91.59\t train_loss: 0.2797\n",
      "train\t epoch:2/3\t batch_index:200\t train_acc: 91.58\t train_loss: 0.2799\n",
      "train\t epoch:2/3\t batch_index:201\t train_acc: 91.58\t train_loss: 0.2798\n",
      "train\t epoch:2/3\t batch_index:202\t train_acc: 91.58\t train_loss: 0.2798\n",
      "train\t epoch:2/3\t batch_index:203\t train_acc: 91.57\t train_loss: 0.2799\n",
      "train\t epoch:2/3\t batch_index:204\t train_acc: 91.57\t train_loss: 0.2798\n",
      "train\t epoch:2/3\t batch_index:205\t train_acc: 91.59\t train_loss: 0.2795\n",
      "train\t epoch:2/3\t batch_index:206\t train_acc: 91.59\t train_loss: 0.2795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\t epoch:2/3\t batch_index:207\t train_acc: 91.59\t train_loss: 0.2794\n",
      "train\t epoch:2/3\t batch_index:208\t train_acc: 91.6\t train_loss: 0.2793\n",
      "train\t epoch:2/3\t batch_index:209\t train_acc: 91.61\t train_loss: 0.2793\n",
      "train\t epoch:2/3\t batch_index:210\t train_acc: 91.6\t train_loss: 0.2793\n",
      "train\t epoch:2/3\t batch_index:211\t train_acc: 91.59\t train_loss: 0.2792\n",
      "train\t epoch:2/3\t batch_index:212\t train_acc: 91.6\t train_loss: 0.2791\n",
      "train\t epoch:2/3\t batch_index:213\t train_acc: 91.58\t train_loss: 0.2792\n",
      "train\t epoch:2/3\t batch_index:214\t train_acc: 91.57\t train_loss: 0.2794\n",
      "train\t epoch:2/3\t batch_index:215\t train_acc: 91.58\t train_loss: 0.2792\n",
      "train\t epoch:2/3\t batch_index:216\t train_acc: 91.57\t train_loss: 0.2793\n",
      "train\t epoch:2/3\t batch_index:217\t train_acc: 91.59\t train_loss: 0.2792\n",
      "train\t epoch:2/3\t batch_index:218\t train_acc: 91.6\t train_loss: 0.279\n",
      "train\t epoch:2/3\t batch_index:219\t train_acc: 91.58\t train_loss: 0.2792\n",
      "train\t epoch:2/3\t batch_index:220\t train_acc: 91.58\t train_loss: 0.2793\n",
      "train\t epoch:2/3\t batch_index:221\t train_acc: 91.56\t train_loss: 0.2796\n",
      "train\t epoch:2/3\t batch_index:222\t train_acc: 91.56\t train_loss: 0.2796\n",
      "train\t epoch:2/3\t batch_index:223\t train_acc: 91.56\t train_loss: 0.2797\n",
      "train\t epoch:2/3\t batch_index:224\t train_acc: 91.57\t train_loss: 0.2797\n",
      "train\t epoch:2/3\t batch_index:225\t train_acc: 91.55\t train_loss: 0.2797\n",
      "train\t epoch:2/3\t batch_index:226\t train_acc: 91.56\t train_loss: 0.2795\n",
      "train\t epoch:2/3\t batch_index:227\t train_acc: 91.56\t train_loss: 0.2796\n",
      "train\t epoch:2/3\t batch_index:228\t train_acc: 91.56\t train_loss: 0.2795\n",
      "train\t epoch:2/3\t batch_index:229\t train_acc: 91.57\t train_loss: 0.2794\n",
      "train\t epoch:2/3\t batch_index:230\t train_acc: 91.57\t train_loss: 0.2792\n",
      "train\t epoch:2/3\t batch_index:231\t train_acc: 91.57\t train_loss: 0.2793\n",
      "train\t epoch:2/3\t batch_index:232\t train_acc: 91.57\t train_loss: 0.2793\n",
      "train\t epoch:2/3\t batch_index:233\t train_acc: 91.57\t train_loss: 0.2793\n",
      "train\t epoch:2/3\t batch_index:234\t train_acc: 91.57\t train_loss: 0.2794\n",
      "train\t epoch:2/3\t batch_index:235\t train_acc: 91.55\t train_loss: 0.2796\n",
      "train\t epoch:2/3\t batch_index:236\t train_acc: 91.54\t train_loss: 0.2797\n",
      "train\t epoch:2/3\t batch_index:237\t train_acc: 91.57\t train_loss: 0.2794\n",
      "train\t epoch:2/3\t batch_index:238\t train_acc: 91.56\t train_loss: 0.2795\n",
      "train\t epoch:2/3\t batch_index:239\t train_acc: 91.55\t train_loss: 0.2795\n",
      "train\t epoch:2/3\t batch_index:240\t train_acc: 91.54\t train_loss: 0.2795\n",
      "train\t epoch:2/3\t batch_index:241\t train_acc: 91.54\t train_loss: 0.2794\n",
      "train\t epoch:2/3\t batch_index:242\t train_acc: 91.53\t train_loss: 0.2795\n",
      "train\t epoch:2/3\t batch_index:243\t train_acc: 91.54\t train_loss: 0.2793\n",
      "train\t epoch:2/3\t batch_index:244\t train_acc: 91.54\t train_loss: 0.2795\n",
      "train\t epoch:2/3\t batch_index:245\t train_acc: 91.54\t train_loss: 0.2795\n",
      "train\t epoch:2/3\t batch_index:246\t train_acc: 91.56\t train_loss: 0.2794\n",
      "train\t epoch:2/3\t batch_index:247\t train_acc: 91.56\t train_loss: 0.2795\n",
      "train\t epoch:2/3\t batch_index:248\t train_acc: 91.55\t train_loss: 0.2795\n",
      "train\t epoch:2/3\t batch_index:249\t train_acc: 91.55\t train_loss: 0.2793\n",
      "train\t epoch:2/3\t batch_index:250\t train_acc: 91.54\t train_loss: 0.2794\n",
      "train\t epoch:2/3\t batch_index:251\t train_acc: 91.54\t train_loss: 0.2794\n",
      "train\t epoch:2/3\t batch_index:252\t train_acc: 91.51\t train_loss: 0.2795\n",
      "train\t epoch:2/3\t batch_index:253\t train_acc: 91.5\t train_loss: 0.2795\n",
      "train\t epoch:2/3\t batch_index:254\t train_acc: 91.49\t train_loss: 0.2796\n",
      "train\t epoch:2/3\t batch_index:255\t train_acc: 91.49\t train_loss: 0.2796\n",
      "train\t epoch:2/3\t batch_index:256\t train_acc: 91.5\t train_loss: 0.2794\n",
      "train\t epoch:2/3\t batch_index:257\t train_acc: 91.49\t train_loss: 0.2793\n",
      "train\t epoch:2/3\t batch_index:258\t train_acc: 91.51\t train_loss: 0.2791\n",
      "train\t epoch:2/3\t batch_index:259\t train_acc: 91.5\t train_loss: 0.2792\n",
      "train\t epoch:2/3\t batch_index:260\t train_acc: 91.49\t train_loss: 0.2792\n",
      "train\t epoch:2/3\t batch_index:261\t train_acc: 91.49\t train_loss: 0.2791\n",
      "train\t epoch:2/3\t batch_index:262\t train_acc: 91.49\t train_loss: 0.2791\n",
      "train\t epoch:2/3\t batch_index:263\t train_acc: 91.49\t train_loss: 0.279\n",
      "train\t epoch:2/3\t batch_index:264\t train_acc: 91.49\t train_loss: 0.279\n",
      "train\t epoch:2/3\t batch_index:265\t train_acc: 91.51\t train_loss: 0.2788\n",
      "train\t epoch:2/3\t batch_index:266\t train_acc: 91.52\t train_loss: 0.2787\n",
      "train\t epoch:2/3\t batch_index:267\t train_acc: 91.53\t train_loss: 0.2785\n",
      "train\t epoch:2/3\t batch_index:268\t train_acc: 91.53\t train_loss: 0.2785\n",
      "train\t epoch:2/3\t batch_index:269\t train_acc: 91.55\t train_loss: 0.2784\n",
      "train\t epoch:2/3\t batch_index:270\t train_acc: 91.54\t train_loss: 0.2784\n",
      "train\t epoch:2/3\t batch_index:271\t train_acc: 91.52\t train_loss: 0.2784\n",
      "train\t epoch:2/3\t batch_index:272\t train_acc: 91.53\t train_loss: 0.2783\n",
      "train\t epoch:2/3\t batch_index:273\t train_acc: 91.52\t train_loss: 0.2783\n",
      "train\t epoch:2/3\t batch_index:274\t train_acc: 91.52\t train_loss: 0.2781\n",
      "train\t epoch:2/3\t batch_index:275\t train_acc: 91.51\t train_loss: 0.2782\n",
      "train\t epoch:2/3\t batch_index:276\t train_acc: 91.51\t train_loss: 0.2781\n",
      "train\t epoch:2/3\t batch_index:277\t train_acc: 91.51\t train_loss: 0.2781\n",
      "train\t epoch:2/3\t batch_index:278\t train_acc: 91.52\t train_loss: 0.2778\n",
      "train\t epoch:2/3\t batch_index:279\t train_acc: 91.54\t train_loss: 0.2777\n",
      "train\t epoch:2/3\t batch_index:280\t train_acc: 91.53\t train_loss: 0.2777\n",
      "train\t epoch:2/3\t batch_index:281\t train_acc: 91.52\t train_loss: 0.2777\n",
      "train\t epoch:2/3\t batch_index:282\t train_acc: 91.52\t train_loss: 0.2776\n",
      "train\t epoch:2/3\t batch_index:283\t train_acc: 91.52\t train_loss: 0.2778\n",
      "train\t epoch:2/3\t batch_index:284\t train_acc: 91.52\t train_loss: 0.2778\n",
      "train\t epoch:2/3\t batch_index:285\t train_acc: 91.52\t train_loss: 0.2778\n",
      "train\t epoch:2/3\t batch_index:286\t train_acc: 91.52\t train_loss: 0.2777\n",
      "train\t epoch:2/3\t batch_index:287\t train_acc: 91.55\t train_loss: 0.2774\n",
      "train\t epoch:2/3\t batch_index:288\t train_acc: 91.54\t train_loss: 0.2774\n",
      "train\t epoch:2/3\t batch_index:289\t train_acc: 91.54\t train_loss: 0.2774\n",
      "train\t epoch:2/3\t batch_index:290\t train_acc: 91.54\t train_loss: 0.2773\n",
      "train\t epoch:2/3\t batch_index:291\t train_acc: 91.53\t train_loss: 0.2772\n",
      "train\t epoch:2/3\t batch_index:292\t train_acc: 91.53\t train_loss: 0.2771\n",
      "train\t epoch:2/3\t batch_index:293\t train_acc: 91.53\t train_loss: 0.2771\n",
      "train\t epoch:2/3\t batch_index:294\t train_acc: 91.52\t train_loss: 0.2772\n",
      "train\t epoch:2/3\t batch_index:295\t train_acc: 91.52\t train_loss: 0.2772\n",
      "train\t epoch:2/3\t batch_index:296\t train_acc: 91.51\t train_loss: 0.2773\n",
      "train\t epoch:2/3\t batch_index:297\t train_acc: 91.51\t train_loss: 0.2775\n",
      "train\t epoch:2/3\t batch_index:298\t train_acc: 91.52\t train_loss: 0.2774\n",
      "train\t epoch:2/3\t batch_index:299\t train_acc: 91.52\t train_loss: 0.2774\n",
      "train\t epoch:2/3\t batch_index:300\t train_acc: 91.52\t train_loss: 0.2773\n",
      "train\t epoch:2/3\t batch_index:301\t train_acc: 91.52\t train_loss: 0.2773\n",
      "train\t epoch:2/3\t batch_index:302\t train_acc: 91.52\t train_loss: 0.2773\n",
      "train\t epoch:2/3\t batch_index:303\t train_acc: 91.52\t train_loss: 0.277\n",
      "train\t epoch:2/3\t batch_index:304\t train_acc: 91.53\t train_loss: 0.277\n",
      "train\t epoch:2/3\t batch_index:305\t train_acc: 91.54\t train_loss: 0.2768\n",
      "val\t epoch:2/3\t batch_index:0\t val_acc: 89.06\t val_loss: 0.286\n",
      "val\t epoch:2/3\t batch_index:1\t val_acc: 89.06\t val_loss: 0.2678\n",
      "val\t epoch:2/3\t batch_index:2\t val_acc: 90.89\t val_loss: 0.254\n",
      "val\t epoch:2/3\t batch_index:3\t val_acc: 90.43\t val_loss: 0.2592\n",
      "val\t epoch:2/3\t batch_index:4\t val_acc: 90.47\t val_loss: 0.2751\n",
      "val\t epoch:2/3\t batch_index:5\t val_acc: 90.76\t val_loss: 0.2773\n",
      "val\t epoch:2/3\t batch_index:6\t val_acc: 91.07\t val_loss: 0.2716\n",
      "val\t epoch:2/3\t batch_index:7\t val_acc: 91.7\t val_loss: 0.2653\n",
      "val\t epoch:2/3\t batch_index:8\t val_acc: 90.97\t val_loss: 0.2712\n",
      "val\t epoch:2/3\t batch_index:9\t val_acc: 90.86\t val_loss: 0.2726\n",
      "val\t epoch:2/3\t batch_index:10\t val_acc: 90.77\t val_loss: 0.2726\n",
      "val\t epoch:2/3\t batch_index:11\t val_acc: 90.76\t val_loss: 0.2731\n",
      "val\t epoch:2/3\t batch_index:12\t val_acc: 90.93\t val_loss: 0.2708\n",
      "val\t epoch:2/3\t batch_index:13\t val_acc: 91.07\t val_loss: 0.2677\n",
      "val\t epoch:2/3\t batch_index:14\t val_acc: 90.73\t val_loss: 0.2711\n",
      "val\t epoch:2/3\t batch_index:15\t val_acc: 90.53\t val_loss: 0.274\n",
      "val\t epoch:2/3\t batch_index:16\t val_acc: 90.44\t val_loss: 0.275\n",
      "val\t epoch:2/3\t batch_index:17\t val_acc: 90.58\t val_loss: 0.2732\n",
      "val\t epoch:2/3\t batch_index:18\t val_acc: 90.62\t val_loss: 0.2723\n",
      "val\t epoch:2/3\t batch_index:19\t val_acc: 90.39\t val_loss: 0.2739\n",
      "val\t epoch:2/3\t batch_index:20\t val_acc: 90.44\t val_loss: 0.2737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val\t epoch:2/3\t batch_index:21\t val_acc: 90.45\t val_loss: 0.2744\n",
      "val\t epoch:2/3\t batch_index:22\t val_acc: 90.56\t val_loss: 0.2724\n",
      "val\t epoch:2/3\t batch_index:23\t val_acc: 90.69\t val_loss: 0.2724\n",
      "val\t epoch:2/3\t batch_index:24\t val_acc: 90.53\t val_loss: 0.2742\n",
      "val\t epoch:2/3\t batch_index:25\t val_acc: 90.41\t val_loss: 0.2761\n",
      "val\t epoch:2/3\t batch_index:26\t val_acc: 90.57\t val_loss: 0.2756\n",
      "val\t epoch:2/3\t batch_index:27\t val_acc: 90.62\t val_loss: 0.2747\n",
      "val\t epoch:2/3\t batch_index:28\t val_acc: 90.52\t val_loss: 0.2766\n",
      "val\t epoch:2/3\t batch_index:29\t val_acc: 90.49\t val_loss: 0.2767\n",
      "val\t epoch:2/3\t batch_index:30\t val_acc: 90.57\t val_loss: 0.276\n",
      "val\t epoch:2/3\t batch_index:31\t val_acc: 90.62\t val_loss: 0.2763\n",
      "val\t epoch:2/3\t batch_index:32\t val_acc: 90.62\t val_loss: 0.2761\n",
      "val\t epoch:2/3\t batch_index:33\t val_acc: 90.67\t val_loss: 0.2749\n",
      "val\t epoch:2/3\t batch_index:34\t val_acc: 90.6\t val_loss: 0.2748\n",
      "val\t epoch:2/3\t batch_index:35\t val_acc: 90.76\t val_loss: 0.2731\n",
      "val\t epoch:2/3\t batch_index:36\t val_acc: 90.62\t val_loss: 0.2747\n",
      "val\t epoch:2/3\t batch_index:37\t val_acc: 90.58\t val_loss: 0.2753\n",
      "val\t epoch:2/3\t batch_index:38\t val_acc: 90.67\t val_loss: 0.2741\n",
      "val\t epoch:2/3\t batch_index:39\t val_acc: 90.68\t val_loss: 0.2742\n",
      "val\t epoch:2/3\t batch_index:40\t val_acc: 90.74\t val_loss: 0.2737\n",
      "val\t epoch:2/3\t batch_index:41\t val_acc: 90.68\t val_loss: 0.2734\n",
      "val\t epoch:2/3\t batch_index:42\t val_acc: 90.7\t val_loss: 0.2728\n",
      "val\t epoch:2/3\t batch_index:43\t val_acc: 90.71\t val_loss: 0.2736\n",
      "val\t epoch:2/3\t batch_index:44\t val_acc: 90.69\t val_loss: 0.2738\n",
      "val\t epoch:2/3\t batch_index:45\t val_acc: 90.69\t val_loss: 0.2735\n",
      "val\t epoch:2/3\t batch_index:46\t val_acc: 90.72\t val_loss: 0.2734\n",
      "val\t epoch:2/3\t batch_index:47\t val_acc: 90.74\t val_loss: 0.274\n",
      "val\t epoch:2/3\t batch_index:48\t val_acc: 90.7\t val_loss: 0.2746\n",
      "val\t epoch:2/3\t batch_index:49\t val_acc: 90.73\t val_loss: 0.2751\n",
      "val\t epoch:2/3\t batch_index:50\t val_acc: 90.73\t val_loss: 0.2754\n",
      "val\t epoch:2/3\t batch_index:51\t val_acc: 90.79\t val_loss: 0.2746\n",
      "val\t epoch:2/3\t batch_index:52\t val_acc: 90.82\t val_loss: 0.2754\n",
      "val\t epoch:2/3\t batch_index:53\t val_acc: 90.83\t val_loss: 0.2759\n",
      "val\t epoch:2/3\t batch_index:54\t val_acc: 90.88\t val_loss: 0.2749\n",
      "val\t epoch:2/3\t batch_index:55\t val_acc: 90.92\t val_loss: 0.2752\n",
      "val\t epoch:2/3\t batch_index:56\t val_acc: 90.87\t val_loss: 0.2752\n",
      "val\t epoch:2/3\t batch_index:57\t val_acc: 90.84\t val_loss: 0.276\n",
      "val\t epoch:2/3\t batch_index:58\t val_acc: 90.85\t val_loss: 0.2759\n",
      "val\t epoch:2/3\t batch_index:59\t val_acc: 90.9\t val_loss: 0.2757\n",
      "val\t epoch:2/3\t batch_index:60\t val_acc: 90.91\t val_loss: 0.2758\n",
      "val\t epoch:2/3\t batch_index:61\t val_acc: 90.94\t val_loss: 0.2759\n",
      "val\t epoch:2/3\t batch_index:62\t val_acc: 90.98\t val_loss: 0.276\n",
      "val\t epoch:2/3\t batch_index:63\t val_acc: 91.03\t val_loss: 0.2755\n",
      "val\t epoch:2/3\t batch_index:64\t val_acc: 91.01\t val_loss: 0.2754\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "        \n",
    "        # Iterate over training dataset\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "        \n",
    "        dataset.set_split(\"train\")\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device = args.device)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "        \n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "            loss_t = loss.item()\n",
    "            \n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # compute accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            status = f\"train\\t epoch:{epoch_index}/{args.num_epochs}\\t batch_index:{batch_index}\\t train_acc: {running_acc:.4}\\t train_loss: {running_loss:.4}\"\n",
    "            print(status)\n",
    "            \n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            # compute the output\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            \n",
    "            \n",
    "            status = f\"val\\t epoch:{epoch_index}/{args.num_epochs}\\t batch_index:{batch_index}\\t val_acc: {running_acc:.4}\\t val_loss: {running_loss:.4}\"\n",
    "            print(status)\n",
    "            \n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        \n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## compute the loss & accuracy on the test set using the best available model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "2it [00:00, 16.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "4it [00:00, 18.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "6it [00:00, 16.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "8it [00:00, 17.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "11it [00:00, 18.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "14it [00:00, 19.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "17it [00:00, 20.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "19it [00:00, 19.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "21it [00:01, 19.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "23it [00:01, 19.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "25it [00:01, 19.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "27it [00:01, 19.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "29it [00:01, 19.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "31it [00:01, 18.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "33it [00:01, 18.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "35it [00:01, 18.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "37it [00:02, 18.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "39it [00:02, 18.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "41it [00:02, 17.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "43it [00:02, 17.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "45it [00:02, 17.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "47it [00:02, 17.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "49it [00:02, 17.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "51it [00:03, 16.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "53it [00:03, 16.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "55it [00:03, 16.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "57it [00:03, 16.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "60it [00:03, 16.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "62it [00:03, 16.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "64it [00:03, 16.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "65it [00:03, 16.83it/s]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in tqdm(enumerate(batch_generator)):\n",
    "    # compute the output\n",
    "    y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.275\n",
      "Test Accuracy: 91.14\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {:.3f}\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {:.2f}\".format(train_state['test_acc']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Prediction (Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def predict_rating(review, classifier, vectorizer, decision_threshold=0.5):\n",
    "    \"\"\"Predict the rating of a review\n",
    "    \n",
    "    Args:\n",
    "        review (str): the text of the review\n",
    "        classifier (ReviewClassifier): the trained model\n",
    "        vectorizer (ReviewVectorizer): the corresponding vectorizer\n",
    "        decision_threshold (float): The numerical boundary which separates the rating classes\n",
    "    \"\"\"\n",
    "    review = preprocess_text(review)\n",
    "    \n",
    "    vectorized_review = torch.tensor(vectorizer.vectorizer(review))\n",
    "    result = classifier(vectorized_review.view(1, -1))\n",
    "    \n",
    "    probability_value = F.sigmoid(result).item()\n",
    "    index = 1\n",
    "    if probability_value < decision_threshold:\n",
    "        index = 0\n",
    "\n",
    "    return vectorizer.rating_vocab.lookup_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sudipa is a beautiful girl -> positive\n"
     ]
    }
   ],
   "source": [
    "test_review = \"Sudipa is a beautiful girl\"\n",
    "\n",
    "classifier = classifier.cpu()\n",
    "prediction = predict_rating(test_review, classifier, vectorizer, decision_threshold=0.5)\n",
    "print(\"{} -> {}\".format(test_review, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7326])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fc1.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Sort weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fc1_weights = classifier.fc1.weight.detach()[0]\n",
    "_, indices = torch.sort(fc1_weights, dim=0, descending=True)\n",
    "indices = indices.numpy().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Top 20 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Influential words in Positive Reviews:\n",
      "--------------------------------------\n",
      "great\n",
      "delicious\n",
      "amazing\n",
      "awesome\n",
      "excellent\n",
      "love\n",
      "vegas\n",
      "fantastic\n",
      "perfect\n",
      "best\n",
      "favorite\n",
      "friendly\n",
      "definitely\n",
      "wonderful\n",
      "loved\n",
      "always\n",
      "yummy\n",
      "helpful\n",
      "highly\n",
      "yum\n"
     ]
    }
   ],
   "source": [
    "print(\"Influential words in Positive Reviews:\")\n",
    "print(\"--------------------------------------\")\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Top 20 negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Influential words in Negative Reviews:\n",
      "--------------------------------------\n",
      "worst\n",
      "bland\n",
      "horrible\n",
      "mediocre\n",
      "rude\n",
      "terrible\n",
      "overpriced\n",
      "not\n",
      "awful\n",
      "ok\n",
      "poor\n",
      "meh\n",
      "dirty\n",
      "nothing\n",
      "disgusting\n",
      "tasteless\n",
      "worse\n",
      "disappointing\n",
      "poorly\n",
      "gross\n"
     ]
    }
   ],
   "source": [
    "print(\"Influential words in Negative Reviews:\")\n",
    "print(\"--------------------------------------\")\n",
    "indices.reverse()\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
